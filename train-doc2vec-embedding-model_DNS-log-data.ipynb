{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build doc2vec embedding model w/ public-domain DNS log data\n",
    "This example builds a vector embedding model from sample dns data using doc2vec.\n",
    "Source data: http://www.secrepo.com/maccdc2012/dns.log.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a vector embedding model using Doc2Vec (from the Gensim library) involves converting text documents into dense vector representations. These vector embeddings capture semantic information about the text, allowing you to use them for tasks such as document similarity, clustering, or classification. Here’s a step-by-step explanation of the process:\n",
    "1. Understanding Doc2Vec\n",
    "\n",
    "Doc2Vec is an extension of Word2Vec, which generates vector embeddings for entire documents or sentences rather than individual words. It introduces the concept of document vectors, which represent each document in a continuous vector space. Doc2Vec uses two primary models:\n",
    "\n",
    "    Distributed Memory (DM): Focuses on predicting a word using both surrounding words and a document vector. It's somewhat similar to how Word2Vec works but also includes the document vector.\n",
    "    Distributed Bag of Words (DBOW): Focuses on predicting the words in a document using the document vector. It's similar to skip-gram in Word2Vec.\n",
    "\n",
    "2. Preparing Data\n",
    "\n",
    "First, you need to organize your text data for training. The data must be preprocessed and tokenized into words. Each document is labeled with a unique ID so that the model can associate each document with its corresponding vector.\n",
    "Steps for preparing data:\n",
    "\n",
    "    Tokenization: Split your text into tokens (words).\n",
    "    Remove stop words: Optionally, remove common stop words.\n",
    "    Label your documents: Doc2Vec requires each document to have a unique label, which can be done using TaggedDocument in Gensim.\n",
    "\n",
    "In this example, each document is tokenized and tagged with a unique ID (e.g., '0', '1', '2')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim\n",
    "#! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do once to get NLTK tokenizers\n",
    "#import nltk\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427935"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DNS log data into a list\n",
    "filename = 'dns.log'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    # Read lines into a list, removing the newline characters\n",
    "    documents = [line.strip() for line in file.readlines()]  # Treat each line as a separate document\n",
    "\n",
    "len(documents)  # How many log entries do we have?  Should be 427K..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tagged documents\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1331901005.510000\\tCWGtK431H9XuaTN4fi\\t192.168.202.100\\t45658\\t192.168.27.203\\t137\\tudp\\t33008\\t*\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\t1\\tC_INTERNET\\t33\\tSRV\\t0\\tNOERROR\\tF\\tF\\tF\\tF\\t1\\t-\\t-\\tF'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]  # Take a look at the raw log data...\\t are tab characters. First row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['1331901005.510000', 'cwgtk431h9xuatn4fi', '192.168.202.100', '45658', '192.168.27.203', '137', 'udp', '33008', '*', '\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00', '1', 'c_internet', '33', 'srv', '0', 'noerror', 'f', 'f', 'f', 'f', '1', '-', '-', 'f'], tags=['0'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0] # Take a look at the tagged data...first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define the model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters:\n",
    "# vector_size: size of the document vectors\n",
    "# window: context window size (how many words before and after to look)\n",
    "# min_count: minimum word frequency\n",
    "# workers: number of CPU cores to use\n",
    "# dm: 1 for Distributed Memory (DM), 0 for Distributed Bag of Words (DBOW)\n",
    "\n",
    "model = Doc2Vec(vector_size=8, window=5, min_count=2, workers=4, dm=1)\n",
    "\n",
    "# Build vocabulary\n",
    "model.build_vocab(tagged_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training the Model\n",
    "\n",
    "Training involves optimizing the model weights by predicting words in context and adjusting the document vectors accordingly. The number of epochs controls how many passes over the training data the model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"doc2vec-embedding-model_DNSv1.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using the Model\n",
    "\n",
    "Once the model is trained, you can use it to infer document vectors, calculate similarity between documents, or perform other downstream tasks.\n",
    "Inferring vectors for new documents:\n",
    "\n",
    "You can infer the vector for new, unseen documents (important for testing and evaluating the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24770531,  0.16860287,  0.07276959, -0.32238778,  0.34391066,\n",
       "       -0.17559938,  0.5203123 , -0.48106503], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing inference\n",
    "\n",
    "# If the model is not already loaded, can load the model like so:\n",
    "# from gensim.models import Doc2Vec\n",
    "model = Doc2Vec.load(\"doc2vec-embedding-model_DNSv1.model\")\n",
    "\n",
    "#new_doc = word_tokenize(\"This is a new document\")\n",
    "# First line from dns.log file...we need something to visually examine\n",
    "new_doc = word_tokenize('1331901005.510000\tCWGtK431H9XuaTN4fi\t192.168.202.100\t45658\t192.168.27.203\t137\tudp\t33008\t*\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\t1\tC_INTERNET\t33\tSRV\t0\tNOERROR\tF\tF\tF\tF\t1\t-\t-\tF')\n",
    "vector = model.infer_vector(new_doc)\n",
    "vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding similar documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('131332', 0.9562419056892395),\n",
       " ('41215', 0.9532216191291809),\n",
       " ('39909', 0.947417140007019),\n",
       " ('234482', 0.9463363289833069),\n",
       " ('49825', 0.9458643198013306)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the vector embeddings to find similar documents\n",
    "similar_docs = model.dv.most_similar([vector], topn=5)\n",
    "similar_docs\n",
    "# We should get an exact match since we used a log entry this model was trained on...investigate why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluating the Model\n",
    "\n",
    "Evaluation involves measuring how well the model’s document vectors capture meaningful semantic relationships. You can test the similarity of document vectors or use them in downstream tasks like clustering or classification.\n",
    "For clustering:\n",
    "\n",
    "    Use clustering algorithms like KMeans, DBSCAN, or others to group documents based on their vector representations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Rework this...we need a better examination of the document vector embeddings\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract document vectors\n",
    "doc_vectors = [model.dv[i] for i in range(len(documents))]\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1331901005.510000\\tCWGtK431H9XuaTN4fi\\t192.168.202.100\\t45658\\t192.168.27.203\\t137\\tudp\\t33008\\t*\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\t1\\tC_INTERNET\\t33\\tSRV\\t0\\tNOERROR\\tF\\tF\\tF\\tF\\t1\\t-\\t-\\tF'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02516805, -0.06995159, -0.108978  , -0.02764933, -0.33432212,\n",
       "       -0.24178472, -0.20091422,  0.00483151], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract document vectors\n",
    "doc_vectors = [model.dv[i] for i in range(len(documents))]\n",
    "\n",
    "doc_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull the documents and the document vectors together into a dataframe so the data is easier to work with\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'document': documents, 'embedding': doc_vectors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427935, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204601</th>\n",
       "      <td>1331922673.920000\\tCjU3fM3OFW6Ms3en2b\\t192.168...</td>\n",
       "      <td>[-0.052768346, 0.061195884, -0.107524134, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271569</th>\n",
       "      <td>1331927876.970000\\tCtdfFj9yL3LSG44tl\\t192.168....</td>\n",
       "      <td>[0.039411146, 0.053052314, 0.00971435, -0.0230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421433</th>\n",
       "      <td>1332014436.070000\\tCAvlcW2Lqg2lkN4Xtj\\t192.168...</td>\n",
       "      <td>[0.080716886, -0.07902379, -0.012096887, -0.10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 document  \\\n",
       "204601  1331922673.920000\\tCjU3fM3OFW6Ms3en2b\\t192.168...   \n",
       "271569  1331927876.970000\\tCtdfFj9yL3LSG44tl\\t192.168....   \n",
       "421433  1332014436.070000\\tCAvlcW2Lqg2lkN4Xtj\\t192.168...   \n",
       "\n",
       "                                                embedding  \n",
       "204601  [-0.052768346, 0.061195884, -0.107524134, -0.0...  \n",
       "271569  [0.039411146, 0.053052314, 0.00971435, -0.0230...  \n",
       "421433  [0.080716886, -0.07902379, -0.012096887, -0.10...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# XXX  Let's try clustering using DBSCAN to determine the number of cluster groups/etc\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Convert the embeddings column to a numpy array\n",
    "embeddings = np.array(df['embedding'].tolist())\n",
    "\n",
    "# Standardize the data (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Calculate the cosine distance matrix\n",
    "cosine_dist_matrix = cosine_distances(embeddings_scaled)\n",
    "\n",
    "# Perform DBSCAN clustering with cosine distance\n",
    "# Parameters to adjust: `eps` (radius of neighborhood), `min_samples` (min points to form a cluster)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='precomputed')\n",
    "df['cluster_group'] = dbscan.fit_predict(cosine_dist_matrix)\n",
    "\n",
    "# Analyzing the results\n",
    "n_clusters = len(set(df['cluster_group'])) - (1 if -1 in df['cluster_group'] else 0)\n",
    "n_noise = list(df['cluster_group']).count(-1)\n",
    "\n",
    "print(f'Estimated number of DBSCAN clusters: {n_clusters}')\n",
    "print(f'Estimated number of noise points: {n_noise}')\n",
    "\n",
    "# If you want to see the first few entries and their clusters\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 823. KiB for an array with shape (105317, 1) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m dbscan_model \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#clusters = dbscan_model.fit_predict(embeddings)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43mdbscan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Add the cluster labels back to the DataFrame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_group\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m clusters\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:474\u001b[0m, in \u001b[0;36mDBSCAN.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute clusters from a data or distance matrix and predict labels.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m        Cluster labels. Noisy samples are given the label -1.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:422\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    420\u001b[0m neighbors_model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# This has worst case O(n^2) memory complexity\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m neighborhoods \u001b[38;5;241m=\u001b[39m \u001b[43mneighbors_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m     n_neighbors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mlen\u001b[39m(neighbors) \u001b[38;5;28;01mfor\u001b[39;00m neighbors \u001b[38;5;129;01min\u001b[39;00m neighborhoods])\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1241\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin.radius_neighbors\u001b[1;34m(self, X, radius, return_distance, sort_results)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     results \u001b[38;5;241m=\u001b[39m neigh_dist, neigh_ind\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     neigh_ind_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunked_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m     results \u001b[38;5;241m=\u001b[39m _to_object_array(neigh_ind_list)\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort_results:\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2181\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2180\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2181\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2182\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[0;32m   2183\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1072\u001b[0m, in \u001b[0;36mRadiusNeighborsMixin._radius_neighbors_reduce_func\u001b[1;34m(self, dist, start, radius, return_distance)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_radius_neighbors_reduce_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, start, radius, return_distance):\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m        The neighbors indices.\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1072\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Dave Sisk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:1072\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_radius_neighbors_reduce_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, dist, start, radius, return_distance):\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    Callback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m        The neighbors indices.\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1072\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dist]\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_distance:\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 823. KiB for an array with shape (105317, 1) and data type int64"
     ]
    }
   ],
   "source": [
    "# Let's try DBSCAN clustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#embeddings = df['embedding'].tolist()\n",
    "\n",
    "# Train the DBSCAN clustering model\n",
    "dbscan_model = DBSCAN(eps=0.25, min_samples=50, metric='cosine')\n",
    "#clusters = dbscan_model.fit_predict(embeddings)\n",
    "clusters = dbscan_model.fit_predict(doc_vectors)\n",
    "\n",
    "# Add the cluster labels back to the DataFrame\n",
    "df['cluster_group'] = clusters\n",
    "\n",
    "# Analyzing the results\n",
    "n_clusters = len(set(df['cluster_group'])) - (1 if -1 in df['cluster_group'] else 0)\n",
    "n_noise = list(df['cluster_group']).count(-1)\n",
    "\n",
    "print(f'Estimated number of DBSCAN clusters: {n_clusters}')\n",
    "print(f'Estimated number of noise points: {n_noise}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "    Prepare the data: Tokenize and tag the documents.\n",
    "    Build the model: Initialize a Doc2Vec model with appropriate hyperparameters.\n",
    "    Train the model: Train the model on the tagged document data.\n",
    "    Infer vectors: Use the trained model to infer vectors for new documents.\n",
    "    Use the vectors: Perform similarity search, clustering, or other tasks using the document vectors.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
