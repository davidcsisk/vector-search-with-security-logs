{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Doc2Vec on Wikipedia articles full content\n",
    "1/13/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the **Document Embedding with Paragraph Vectors** paper, http://arxiv.org/abs/1507.07998, and it also adds on to this notebook from Gensim: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb\n",
    "\n",
    "In that paper, the authors only showed results from the DBOW (\"distributed bag of words\") mode, trained on the English Wikipedia. Here we replicate this experiment using not only DBOW, but also the DM (\"distributed memory\") mode of the Paragraph Vector algorithm aka Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary modules and set up logging. The code below assumes Python 3.7+ and Gensim 4.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "from gensim.corpora.wikicorpus import WikiCorpus, tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [here](http://download.wikimedia.org/enwiki/latest). You want the file named `enwiki-latest-pages-articles.xml.bz2`.\n",
    "\n",
    "Second, convert that Wikipedia article dump from the arcane Wikimedia XML format into a plain text file. This will make the subsequent training faster and also allow easy inspection of the data = \"input eyeballing\".\n",
    "\n",
    "We'll preprocess each article at the same time, normalizing its text to lowercase, splitting into tokens, etc. Below I use a regexp tokenizer that simply looks for alphabetic sequences as tokens. But feel free to adapt the text preprocessing to your own domain. High quality preprocessing is often critical for the final pipeline accuracy – garbage in, garbage out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\utils.py:1332: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n",
      "2025-01-08 14:12:20,778 : INFO : processing article #0: 'Anarchism' (6790 tokens)\n",
      "2025-01-08 14:40:58,152 : INFO : processing article #500000: 'Dora Riparia' (364 tokens)\n",
      "2025-01-08 15:00:30,204 : INFO : processing article #1000000: 'Nashville School of Law' (852 tokens)\n",
      "2025-01-08 15:18:02,203 : INFO : processing article #1500000: 'Leetonia, Hibbing, Minnesota' (58 tokens)\n",
      "2025-01-08 15:36:12,284 : INFO : processing article #2000000: 'The Next Food Network Star season 4' (2408 tokens)\n",
      "2025-01-08 15:54:37,440 : INFO : processing article #2500000: 'Star Trek: Nero' (53 tokens)\n",
      "2025-01-08 16:13:53,773 : INFO : processing article #3000000: 'David Berni' (534 tokens)\n",
      "2025-01-08 16:34:01,876 : INFO : processing article #3500000: 'Ophichthus mystacinus' (59 tokens)\n",
      "2025-01-08 16:53:28,328 : INFO : processing article #4000000: 'Robert Harington, 3rd Baron Harington' (268 tokens)\n",
      "2025-01-08 17:13:54,948 : INFO : processing article #4500000: 'Melantho (1812 ship)' (333 tokens)\n",
      "2025-01-08 17:36:22,858 : INFO : processing article #5000000: 'The Family (Schiele)' (402 tokens)\n",
      "2025-01-08 17:59:39,004 : INFO : processing article #5500000: '1979 Melbourne Cup' (254 tokens)\n",
      "2025-01-08 18:07:57,354 : INFO : finished iterating over Wikipedia corpus of 5677878 documents with 3402712129 positions (total 24255326 articles, 3484516900 positions before pruning articles shorter than 50 words)\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiCorpus(\n",
    "    \"enwiki-latest-pages-articles.xml.bz2\",  # path to the file you downloaded above\n",
    "    tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n",
    "    metadata=True,  # also return the article titles and ids when parsing\n",
    "    dictionary={},  # don't start processing the data yet\n",
    ")\n",
    "\n",
    "with smart_open.open(\"wiki.txt.gz\", \"w\", encoding='utf8') as fout:\n",
    "    for article_no, (content, (page_id, title)) in enumerate(wiki.get_texts()):\n",
    "        title = ' '.join(title.split())\n",
    "        if article_no % 500000 == 0:\n",
    "            logging.info(\"processing article #%i: %r (%i tokens)\", article_no, title, len(content))\n",
    "        fout.write(f\"{title}\\t{' '.join(content)}\\n\")  # title_of_article [TAB] words of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took about 1 hour and created a new ~5.8 GB file named `wiki.txt.gz`. Note the output text was transparently compressed into `.gz` (GZIP) right away, using the [smart_open](https://github.com/RaRe-Technologies/smart_open) library, to save on disk space.\n",
    "\n",
    "Note that `wiki.txt.gz` has been saved here so you can skip that whole step if desired: \n",
    "https://mega.nz/file/L2YzUQAD#b_2pkaWHkFUdWoTa-EypJmVXWetVeqsi1fVlMeMeAzk\n",
    "\n",
    "Next we'll set up a document stream to load the preprocessed articles from `wiki.txt.gz` one by one, in the format expected by Doc2Vec, ready for training. We don't want to load everything into RAM at once, because that would blow up the memory. And it is not necessary – Gensim can handle streamed input training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiCorpus:\n",
    "    def __init__(self, wiki_text_path):\n",
    "        self.wiki_text_path = wiki_text_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in smart_open.open(self.wiki_text_path, encoding='utf8'):\n",
    "            title, words = line.split('\\t')\n",
    "            yield TaggedDocument(words=words.split(), tags=[title])\n",
    "\n",
    "documents = TaggedWikiCorpus('wiki.txt.gz')  # A streamed iterable; nothing in RAM yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism'] :  anarchism is political philosophy and movement that is against all forms of authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy typically including the state and capitalism anarchism advocates for the replacement of the state with stateless societies and voluntary free associations historically left wing ……… sources further reading criticism of philosophical anarchism defence of philosophical anarchism stating that both kinds of anarchism philosophical and political anarchism are philosophical and political claims anarchistic popular fiction novel an argument for philosophical anarchism external links anarchy archives an online research center on the history and theory of anarchism\n"
     ]
    }
   ],
   "source": [
    "# Load and print the first preprocessed Wikipedia document, as a sanity check = \"input eyeballing\".\n",
    "first_doc = next(iter(documents))\n",
    "print(first_doc.tags, ': ', ' '.join(first_doc.words[:50] + ['………'] + first_doc.words[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document seems legit so let's move on to finally training some Doc2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper had a vocabulary size of 915,715 word types, so we'll try to match it by setting `max_final_vocab` to 1,000,000 in the Doc2vec constructor.\n",
    "\n",
    "Other critical parameters were left unspecified in the paper, so we'll go with a window size of eight (a prediction window of 8 tokens to either side). It looks like the authors tried vector dimensionality of 100, 300, 1,000 & 10,000 in the paper (with 10k dims performing the best), but I'll only train with 200 dimensions here, to keep the RAM in check on my laptop.\n",
    "\n",
    "Feel free to tinker with these values yourself if you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 07:58:52,387 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t20>', 'datetime': '2025-01-09T07:58:52.387111', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "2025-01-09 07:58:52,389 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t20>', 'datetime': '2025-01-09T07:58:52.389132', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "workers = 20  # multiprocessing.cpu_count() - 1  # leave one core for the OS & other stuff\n",
    "\n",
    "# PV-DBOW: paragraph vector in distributed bag of words mode\n",
    "model_dbow = Doc2Vec(\n",
    "    dm=0, dbow_words=1,  # dbow_words=1 to train word vectors at the same time too, not only DBOW\n",
    "    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    ")\n",
    "\n",
    "# PV-DM: paragraph vector in distributed memory mode\n",
    "model_dm = Doc2Vec(\n",
    "    dm=1, dm_mean=1,  # use average of context word vectors to train DM\n",
    "    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one pass through the Wikipedia corpus, to collect the 1M vocabulary and initialize the doc2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 07:59:02,357 : INFO : collecting all words and their counts\n",
      "2025-01-09 07:59:02,371 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-01-09 08:02:38,158 : INFO : PROGRESS: at example #500000, processed 690662809 words (3200658 words/s), 3299120 word types, 500000 tags\n",
      "2025-01-09 08:04:49,139 : INFO : PROGRESS: at example #1000000, processed 1074431339 words (2929984 words/s), 4585092 word types, 1000000 tags\n",
      "2025-01-09 08:06:34,302 : INFO : PROGRESS: at example #1500000, processed 1375135884 words (2859418 words/s), 5544766 word types, 1500000 tags\n",
      "2025-01-09 08:08:02,967 : INFO : PROGRESS: at example #2000000, processed 1632207736 words (2899398 words/s), 6327024 word types, 2000000 tags\n",
      "2025-01-09 08:09:29,755 : INFO : PROGRESS: at example #2500000, processed 1884101404 words (2902444 words/s), 7096120 word types, 2500000 tags\n",
      "2025-01-09 08:10:56,557 : INFO : PROGRESS: at example #3000000, processed 2132714040 words (2864139 words/s), 7838271 word types, 3000000 tags\n",
      "2025-01-09 08:12:21,964 : INFO : PROGRESS: at example #3500000, processed 2377500003 words (2866138 words/s), 8524028 word types, 3500000 tags\n",
      "2025-01-09 08:13:44,382 : INFO : PROGRESS: at example #4000000, processed 2612596403 words (2852485 words/s), 9148891 word types, 4000000 tags\n",
      "2025-01-09 08:15:06,401 : INFO : PROGRESS: at example #4500000, processed 2840165350 words (2774651 words/s), 9766788 word types, 4500000 tags\n",
      "2025-01-09 08:16:29,817 : INFO : PROGRESS: at example #5000000, processed 3078365900 words (2855595 words/s), 10396521 word types, 5000000 tags\n",
      "2025-01-09 08:17:56,780 : INFO : PROGRESS: at example #5500000, processed 3318917195 words (2766158 words/s), 11005832 word types, 5500000 tags\n",
      "2025-01-09 08:18:52,382 : INFO : collected 11208412 word types and 5677878 unique tags from a corpus of 5677878 examples and 3402712129 words\n",
      "2025-01-09 08:18:57,549 : INFO : Doc2Vec lifecycle event {'msg': 'max_final_vocab=1000000 and min_count=5 resulted in calc_min_count=27, effective_min_count=27', 'datetime': '2025-01-09T08:18:57.548888', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-09 08:18:57,549 : INFO : Creating a fresh vocabulary\n",
      "2025-01-09 08:19:02,514 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 retains 977793 unique words (8.72% of original 11208412, drops 10230619)', 'datetime': '2025-01-09T08:19:02.514381', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-09 08:19:02,515 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 leaves 3370281530 word corpus (99.05% of original 3402712129, drops 32430599)', 'datetime': '2025-01-09T08:19:02.515383', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-09 08:19:06,781 : INFO : deleting the raw counts dictionary of 11208412 items\n",
      "2025-01-09 08:19:06,942 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2025-01-09 08:19:06,942 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2741878554.629451 word corpus (81.4%% of prior 3370281530)', 'datetime': '2025-01-09T08:19:06.942037', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2025-01-09 08:19:14,239 : INFO : estimated required memory for 977793 words and 200 dimensions: 7731243300 bytes\n",
      "2025-01-09 08:19:14,240 : INFO : resetting layer weights\n",
      "2025-01-09 08:19:18,734 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t20>\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t20>\n"
     ]
    }
   ],
   "source": [
    "model_dbow.build_vocab(documents, progress_per=500000)\n",
    "print(model_dbow)\n",
    "\n",
    "# Save some time by copying the vocabulary structures from the DBOW model to the DM model.\n",
    "# Both models are built on top of exactly the same data, so there's no need to repeat the vocab-building step.\n",
    "model_dm.reset_from(model_dbow)\n",
    "print(model_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to train Doc2Vec on the entirety of the English Wikipedia. **Warning!** Training this DBOW model takes ~14 hours, and DM ~6 hours, on my 2020 Linux machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 08:33:05,561 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 20 workers on 977793 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2025-01-09T08:33:05.561594', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-01-09 08:33:06,601 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 227430 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 09:03:06,646 : INFO : EPOCH 0 - PROGRESS: at 10.23% examples, 336596 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 09:33:06,660 : INFO : EPOCH 0 - PROGRESS: at 31.01% examples, 335250 words/s, in_qsize 39, out_qsize 1\n",
      "2025-01-09 10:03:06,671 : INFO : EPOCH 0 - PROGRESS: at 57.43% examples, 334903 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 10:33:06,674 : INFO : EPOCH 0 - PROGRESS: at 85.43% examples, 334998 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 10:48:50,429 : INFO : EPOCH 0: training on 3402712129 raw words (2729419089 effective words) took 8144.9s, 335110 effective words/s\n",
      "2025-01-09 10:48:51,447 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 247945 words/s, in_qsize 39, out_qsize 1\n",
      "2025-01-09 11:18:51,451 : INFO : EPOCH 1 - PROGRESS: at 10.61% examples, 344716 words/s, in_qsize 39, out_qsize 1\n",
      "2025-01-09 11:48:51,456 : INFO : EPOCH 1 - PROGRESS: at 32.20% examples, 342607 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 12:18:51,490 : INFO : EPOCH 1 - PROGRESS: at 58.73% examples, 340200 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 12:48:51,509 : INFO : EPOCH 1 - PROGRESS: at 86.95% examples, 339527 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 13:02:53,168 : INFO : EPOCH 1: training on 3402712129 raw words (2729414500 effective words) took 8042.7s, 339364 effective words/s\n",
      "2025-01-09 13:02:54,180 : INFO : EPOCH 2 - PROGRESS: at 0.00% examples, 278888 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 13:32:54,216 : INFO : EPOCH 2 - PROGRESS: at 10.63% examples, 345268 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 14:02:54,255 : INFO : EPOCH 2 - PROGRESS: at 32.26% examples, 343000 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 14:32:54,269 : INFO : EPOCH 2 - PROGRESS: at 59.00% examples, 341394 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 15:02:54,279 : INFO : EPOCH 2 - PROGRESS: at 87.37% examples, 340769 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 15:16:28,744 : INFO : EPOCH 2: training on 3402712129 raw words (2729421510 effective words) took 8015.6s, 340515 effective words/s\n",
      "2025-01-09 15:16:29,761 : INFO : EPOCH 3 - PROGRESS: at 0.00% examples, 249060 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 15:46:29,781 : INFO : EPOCH 3 - PROGRESS: at 10.64% examples, 345480 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 16:16:29,792 : INFO : EPOCH 3 - PROGRESS: at 32.43% examples, 344068 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 16:46:29,796 : INFO : EPOCH 3 - PROGRESS: at 59.42% examples, 343149 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 17:16:29,807 : INFO : EPOCH 3 - PROGRESS: at 87.85% examples, 342290 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 17:29:27,585 : INFO : EPOCH 3: training on 3402712129 raw words (2729438300 effective words) took 7978.8s, 342085 effective words/s\n",
      "2025-01-09 17:29:28,599 : INFO : EPOCH 4 - PROGRESS: at 0.00% examples, 260988 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 17:59:28,604 : INFO : EPOCH 4 - PROGRESS: at 10.62% examples, 345047 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 18:29:28,625 : INFO : EPOCH 4 - PROGRESS: at 32.31% examples, 343325 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 18:59:28,635 : INFO : EPOCH 4 - PROGRESS: at 59.04% examples, 341586 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 19:29:28,676 : INFO : EPOCH 4 - PROGRESS: at 86.81% examples, 339131 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 19:44:02,847 : INFO : EPOCH 4: training on 3402712129 raw words (2729419407 effective words) took 8075.3s, 337998 effective words/s\n",
      "2025-01-09 19:44:03,859 : INFO : EPOCH 5 - PROGRESS: at 0.00% examples, 161693 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 20:14:03,877 : INFO : EPOCH 5 - PROGRESS: at 10.29% examples, 337957 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 20:44:03,916 : INFO : EPOCH 5 - PROGRESS: at 30.79% examples, 333803 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 21:14:03,919 : INFO : EPOCH 5 - PROGRESS: at 56.13% examples, 329720 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 21:44:03,948 : INFO : EPOCH 5 - PROGRESS: at 83.03% examples, 327666 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 22:02:01,169 : INFO : EPOCH 5: training on 3402712129 raw words (2729438149 effective words) took 8278.3s, 329709 effective words/s\n",
      "2025-01-09 22:02:02,184 : INFO : EPOCH 6 - PROGRESS: at 0.00% examples, 287191 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-09 22:32:02,188 : INFO : EPOCH 6 - PROGRESS: at 10.87% examples, 350221 words/s, in_qsize 38, out_qsize 0\n",
      "2025-01-09 23:02:02,220 : INFO : EPOCH 6 - PROGRESS: at 32.94% examples, 347388 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-09 23:32:02,236 : INFO : EPOCH 6 - PROGRESS: at 60.02% examples, 345687 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 00:02:02,275 : INFO : EPOCH 6 - PROGRESS: at 88.82% examples, 345237 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 00:13:45,387 : INFO : EPOCH 6: training on 3402712129 raw words (2729417596 effective words) took 7904.2s, 345312 effective words/s\n",
      "2025-01-10 00:13:46,405 : INFO : EPOCH 7 - PROGRESS: at 0.00% examples, 290511 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 00:43:46,417 : INFO : EPOCH 7 - PROGRESS: at 11.19% examples, 356747 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 01:13:46,424 : INFO : EPOCH 7 - PROGRESS: at 33.92% examples, 353809 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 01:43:46,433 : INFO : EPOCH 7 - PROGRESS: at 61.59% examples, 352264 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 02:13:46,453 : INFO : EPOCH 7 - PROGRESS: at 90.94% examples, 351456 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 02:23:13,946 : INFO : EPOCH 7: training on 3402712129 raw words (2729396961 effective words) took 7768.6s, 351339 effective words/s\n",
      "2025-01-10 02:23:14,991 : INFO : EPOCH 8 - PROGRESS: at 0.00% examples, 275619 words/s, in_qsize 36, out_qsize 1\n",
      "2025-01-10 02:53:15,016 : INFO : EPOCH 8 - PROGRESS: at 11.18% examples, 356500 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 03:23:15,027 : INFO : EPOCH 8 - PROGRESS: at 33.87% examples, 353461 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 03:53:15,052 : INFO : EPOCH 8 - PROGRESS: at 61.51% examples, 351951 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 04:23:15,067 : INFO : EPOCH 8 - PROGRESS: at 90.80% examples, 351038 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 04:32:54,792 : INFO : EPOCH 8: training on 3402712129 raw words (2729434699 effective words) took 7780.8s, 350790 effective words/s\n",
      "2025-01-10 04:32:55,833 : INFO : EPOCH 9 - PROGRESS: at 0.00% examples, 251172 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 05:02:55,834 : INFO : EPOCH 9 - PROGRESS: at 11.07% examples, 354220 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 05:32:55,834 : INFO : EPOCH 9 - PROGRESS: at 33.33% examples, 349936 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 06:02:55,841 : INFO : EPOCH 9 - PROGRESS: at 60.84% examples, 349219 words/s, in_qsize 38, out_qsize 1\n",
      "2025-01-10 06:32:55,845 : INFO : EPOCH 9 - PROGRESS: at 90.03% examples, 348850 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 06:43:21,062 : INFO : EPOCH 9: training on 3402712129 raw words (2729411347 effective words) took 7826.3s, 348750 effective words/s\n",
      "2025-01-10 06:43:21,063 : INFO : Doc2Vec lifecycle event {'msg': 'training on 34027121290 raw words (27294211558 effective words) took 79815.5s, 341966 effective words/s', 'datetime': '2025-01-10T06:43:21.063777', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-01-10 06:43:21,064 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_wikipedia_dbow.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-01-10T06:43:21.064778', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "2025-01-10 06:43:21,065 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dbow.model.dv.vectors.npy\n",
      "2025-01-10 06:43:25,489 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dbow.model.wv.vectors.npy\n",
      "2025-01-10 06:43:26,230 : INFO : storing np array 'syn1neg' to doc2vec_wikipedia_dbow.model.syn1neg.npy\n",
      "2025-01-10 06:43:26,966 : INFO : not storing attribute cum_table\n",
      "2025-01-10 06:43:31,190 : INFO : saved doc2vec_wikipedia_dbow.model\n"
     ]
    }
   ],
   "source": [
    "# Train DBOW doc2vec incl. word vectors.\n",
    "# Report progress every ½ hour.\n",
    "# NOTE: This runs for ~20 hours\n",
    "model_dbow.train(documents, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=30*60)\n",
    "model_dbow.save('doc2vec_wikipedia_dbow.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 07:04:51,333 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 20 workers on 977793 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2025-01-10T07:04:51.333385', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-01-10 07:04:52,355 : INFO : EPOCH 0 - PROGRESS: at 0.01% examples, 1110553 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 07:34:52,357 : INFO : EPOCH 0 - PROGRESS: at 51.41% examples, 929821 words/s, in_qsize 38, out_qsize 1\n",
      "2025-01-10 07:56:59,321 : INFO : EPOCH 0: training on 3402712129 raw words (2729432921 effective words) took 3128.0s, 872588 effective words/s\n",
      "2025-01-10 07:57:00,340 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 1063460 words/s, in_qsize 0, out_qsize 1\n",
      "2025-01-10 08:27:00,358 : INFO : EPOCH 1 - PROGRESS: at 51.65% examples, 932958 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 08:49:01,312 : INFO : EPOCH 1: training on 3402712129 raw words (2729421858 effective words) took 3122.0s, 874259 effective words/s\n",
      "2025-01-10 08:49:02,323 : INFO : EPOCH 2 - PROGRESS: at 0.01% examples, 1240406 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 09:19:02,336 : INFO : EPOCH 2 - PROGRESS: at 51.08% examples, 925391 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 09:41:15,591 : INFO : EPOCH 2: training on 3402712129 raw words (2729431039 effective words) took 3134.3s, 870834 effective words/s\n",
      "2025-01-10 09:41:16,602 : INFO : EPOCH 3 - PROGRESS: at 0.01% examples, 1228050 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 10:11:16,630 : INFO : EPOCH 3 - PROGRESS: at 51.33% examples, 928695 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 10:33:31,221 : INFO : EPOCH 3: training on 3402712129 raw words (2729390088 effective words) took 3135.6s, 870446 effective words/s\n",
      "2025-01-10 10:33:32,231 : INFO : EPOCH 4 - PROGRESS: at 0.01% examples, 1227045 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 11:03:32,233 : INFO : EPOCH 4 - PROGRESS: at 51.51% examples, 931168 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 11:25:34,222 : INFO : EPOCH 4: training on 3402712129 raw words (2729418749 effective words) took 3123.0s, 873975 effective words/s\n",
      "2025-01-10 11:25:35,237 : INFO : EPOCH 5 - PROGRESS: at 0.01% examples, 1227311 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 11:55:35,239 : INFO : EPOCH 5 - PROGRESS: at 51.87% examples, 935865 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 12:17:27,210 : INFO : EPOCH 5: training on 3402712129 raw words (2729420905 effective words) took 3113.0s, 876788 effective words/s\n",
      "2025-01-10 12:17:28,221 : INFO : EPOCH 6 - PROGRESS: at 0.01% examples, 1265015 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 12:47:28,226 : INFO : EPOCH 6 - PROGRESS: at 51.45% examples, 930330 words/s, in_qsize 40, out_qsize 0\n",
      "2025-01-10 13:09:28,190 : INFO : EPOCH 6: training on 3402712129 raw words (2729416644 effective words) took 3121.0s, 874540 effective words/s\n",
      "2025-01-10 13:09:29,200 : INFO : EPOCH 7 - PROGRESS: at 0.01% examples, 1191510 words/s, in_qsize 0, out_qsize 1\n",
      "2025-01-10 13:39:29,212 : INFO : EPOCH 7 - PROGRESS: at 51.24% examples, 927486 words/s, in_qsize 39, out_qsize 1\n",
      "2025-01-10 14:01:43,927 : INFO : EPOCH 7: training on 3402712129 raw words (2729406566 effective words) took 3135.7s, 870422 effective words/s\n",
      "2025-01-10 14:01:44,935 : INFO : EPOCH 8 - PROGRESS: at 0.01% examples, 1262800 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 14:31:44,938 : INFO : EPOCH 8 - PROGRESS: at 51.75% examples, 934343 words/s, in_qsize 39, out_qsize 0\n",
      "2025-01-10 14:53:35,076 : INFO : EPOCH 8: training on 3402712129 raw words (2729423917 effective words) took 3111.1s, 877306 effective words/s\n",
      "2025-01-10 14:53:36,083 : INFO : EPOCH 9 - PROGRESS: at 0.01% examples, 1235369 words/s, in_qsize 0, out_qsize 0\n",
      "2025-01-10 15:23:36,094 : INFO : EPOCH 9 - PROGRESS: at 52.08% examples, 938739 words/s, in_qsize 39, out_qsize 1\n",
      "2025-01-10 15:45:15,582 : INFO : EPOCH 9: training on 3402712129 raw words (2729407057 effective words) took 3100.5s, 880312 effective words/s\n",
      "2025-01-10 15:45:15,583 : INFO : Doc2Vec lifecycle event {'msg': 'training on 34027121290 raw words (27294169744 effective words) took 31224.2s, 874134 effective words/s', 'datetime': '2025-01-10T15:45:15.583058', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2025-01-10 15:45:15,584 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_wikipedia_dm.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-01-10T15:45:15.584017', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "2025-01-10 15:45:15,584 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.dv.vectors.npy\n",
      "2025-01-10 15:45:20,044 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.wv.vectors.npy\n",
      "2025-01-10 15:45:20,815 : INFO : storing np array 'syn1neg' to doc2vec_wikipedia_dm.model.syn1neg.npy\n",
      "2025-01-10 15:45:21,588 : INFO : not storing attribute cum_table\n",
      "2025-01-10 15:45:26,008 : INFO : saved doc2vec_wikipedia_dm.model\n"
     ]
    }
   ],
   "source": [
    "# Train DM doc2vec.\n",
    "# NOTE: This runs for ~12 hours\n",
    "model_dm.train(documents, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=30*60)\n",
    "model_dm.save('doc2vec_wikipedia_dm.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models have been saved to the url's below so you can avoid this 1-2 days of training compute time if you are just looking to experiment. The models are around 5.4Gb in size each.\n",
    "- doc2vec_wikipedia_dbow-model.zip: https://mega.nz/file/i6pVWQRD#gIdgXKJG5gEjRBZ2BDW_XlklBUtJb81a9EnlUrltnro\n",
    "- doc2vec_wikipedia_dm-model.zip: https://mega.nz/file/ivwDmQYL#G1vmS8jJNpoDWf09mCspNsRzNzGFmqk1UPtTGjN7gBo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already trained or downloaded/unzipped the models and you are picking up here, run the first cell with the imports and then load the models below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 14:36:32,557 : INFO : loading Doc2Vec object from doc2vec_wikipedia_dbow.model\n",
      "2025-01-13 14:36:36,399 : INFO : loading dv recursively from doc2vec_wikipedia_dbow.model.dv.* with mmap=None\n",
      "2025-01-13 14:36:36,400 : INFO : loading vectors from doc2vec_wikipedia_dbow.model.dv.vectors.npy with mmap=None\n",
      "2025-01-13 14:36:37,770 : INFO : loading wv recursively from doc2vec_wikipedia_dbow.model.wv.* with mmap=None\n",
      "2025-01-13 14:36:37,770 : INFO : loading vectors from doc2vec_wikipedia_dbow.model.wv.vectors.npy with mmap=None\n",
      "2025-01-13 14:36:37,995 : INFO : loading syn1neg from doc2vec_wikipedia_dbow.model.syn1neg.npy with mmap=None\n",
      "2025-01-13 14:36:38,227 : INFO : setting ignored attribute cum_table to None\n",
      "2025-01-13 14:36:44,360 : INFO : Doc2Vec lifecycle event {'fname': 'doc2vec_wikipedia_dbow.model', 'datetime': '2025-01-13T14:36:44.360213', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'}\n",
      "2025-01-13 14:36:44,773 : INFO : loading Doc2Vec object from doc2vec_wikipedia_dm.model\n",
      "2025-01-13 14:36:48,677 : INFO : loading dv recursively from doc2vec_wikipedia_dm.model.dv.* with mmap=None\n",
      "2025-01-13 14:36:48,678 : INFO : loading vectors from doc2vec_wikipedia_dm.model.dv.vectors.npy with mmap=None\n",
      "2025-01-13 14:36:50,029 : INFO : loading wv recursively from doc2vec_wikipedia_dm.model.wv.* with mmap=None\n",
      "2025-01-13 14:36:50,030 : INFO : loading vectors from doc2vec_wikipedia_dm.model.wv.vectors.npy with mmap=None\n",
      "2025-01-13 14:36:50,271 : INFO : loading syn1neg from doc2vec_wikipedia_dm.model.syn1neg.npy with mmap=None\n",
      "2025-01-13 14:36:50,505 : INFO : setting ignored attribute cum_table to None\n",
      "2025-01-13 14:36:57,014 : INFO : Doc2Vec lifecycle event {'fname': 'doc2vec_wikipedia_dm.model', 'datetime': '2025-01-13T14:36:57.014251', 'gensim': '4.3.3', 'python': '3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vec_wikipedia_dbow.model')\n",
    "model_dm = Doc2Vec.load('doc2vec_wikipedia_dm.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's test both models! The DBOW model shows similar results as the original paper.\n",
    "\n",
    "First, calculate the most similar Wikipedia articles to the \"Machine learning\" article. The calculated word vectors and document vectors are stored separately, in `model.wv` and `model.dv` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Supervised learning', 0.7331087589263916),\n",
      " ('Neural network (machine learning)', 0.7137694358825684),\n",
      " ('Boosting (machine learning)', 0.7093158960342407),\n",
      " ('Pattern recognition', 0.703530490398407),\n",
      " ('Symbolic artificial intelligence', 0.6911135315895081),\n",
      " ('Liang Zhao', 0.6848805546760559),\n",
      " ('Feature selection', 0.6822749376296997),\n",
      " ('Data mining', 0.6801027655601501),\n",
      " ('Linear classifier', 0.6765809059143066),\n",
      " ('Deep learning', 0.6754170060157776),\n",
      " ('Neural network software', 0.6663535833358765),\n",
      " ('Support vector machine', 0.6654434204101562),\n",
      " ('Multi-task learning', 0.6646270751953125),\n",
      " ('Outline of computer science', 0.6641519069671631),\n",
      " ('Statistical assumption', 0.6632416248321533),\n",
      " ('Bayesian network', 0.6610164642333984),\n",
      " ('Computer scientist', 0.657122015953064),\n",
      " ('Image segmentation', 0.6535505056381226),\n",
      " ('Training, validation, and test data sets', 0.6525720357894897),\n",
      " ('Early stopping', 0.6518106460571289)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Supervised learning', 0.6940938234329224),\n",
      " ('Pattern recognition', 0.6841362714767456),\n",
      " ('Neural network (machine learning)', 0.677743136882782),\n",
      " ('Meta-learning (computer science)', 0.6570912599563599),\n",
      " ('Anomaly detection', 0.6562423706054688),\n",
      " ('Multi-task learning', 0.6456139087677002),\n",
      " ('Feature learning', 0.6430816054344177),\n",
      " ('Explainable artificial intelligence', 0.6397790908813477),\n",
      " ('Data preprocessing', 0.6207960247993469),\n",
      " ('Deep learning', 0.6205981373786926),\n",
      " ('Predictive Model Markup Language', 0.6205079555511475),\n",
      " ('Feature selection', 0.6174596548080444),\n",
      " ('Symbolic artificial intelligence', 0.6173217296600342),\n",
      " ('Ensemble learning', 0.6156730651855469),\n",
      " ('Overfitting', 0.6140542030334473),\n",
      " ('Data analysis for fraud detection', 0.6133849024772644),\n",
      " ('Linear classifier', 0.6129390597343445),\n",
      " ('Automatic image annotation', 0.6110525727272034),\n",
      " ('Multiclass classification', 0.6102665066719055),\n",
      " ('Inductive bias', 0.6089637875556946)]\n"
     ]
    }
   ],
   "source": [
    "for model in [model_dbow, model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both results seem similar and match the results from the paper's Table 1, although not exactly. This is because we don't know the exact parameters of the original implementation (see above). And also because we're training the model 7 years later and the Wikipedia content has changed in the meantime.\n",
    "\n",
    "Now following the paper's Table 2a), let's calculate the most similar Wikipedia entries to \"Lady Gaga\" using Paragraph Vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Katy Perry', 0.7451617121696472),\n",
      " ('Ariana Grande', 0.730435848236084),\n",
      " ('Adele', 0.7103729248046875),\n",
      " ('Nicki Minaj', 0.7018465399742126),\n",
      " ('Miley Cyrus', 0.6926740407943726),\n",
      " ('Taylor Swift', 0.6914724111557007),\n",
      " ('Demi Lovato', 0.676815927028656),\n",
      " ('Selena Gomez', 0.6723569631576538),\n",
      " ('Ellie Goulding', 0.670768141746521),\n",
      " ('Harry Styles', 0.6642683148384094)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Born This Way (album)', 0.6678953766822815),\n",
      " ('Artpop', 0.649540364742279),\n",
      " ('Beautiful, Dirty, Rich', 0.6354315876960754),\n",
      " ('Lady Gaga videography', 0.6259204745292664),\n",
      " ('Jennifer Lopez', 0.6139587759971619),\n",
      " ('Lady Gaga discography', 0.6118119359016418),\n",
      " ('Madonna', 0.6114965677261353),\n",
      " ('Katy Perry', 0.6079288721084595),\n",
      " ('Selena Gomez', 0.5989758968353271),\n",
      " ('Nicki Minaj', 0.5984588861465454)]\n"
     ]
    }
   ],
   "source": [
    "for model in [model_dbow, model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Lady Gaga\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The DBOW results are in line with what the paper shows in Table 2a), revealing similar singers in the U.S.\n",
    "\n",
    "Interestingly, the DM results seem to capture more \"fact about Lady Gaga\" (her albums, trivia), whereas DBOW recovered \"similar artists\".\n",
    "\n",
    "**Finally, let's do some of the wilder arithmetics that vectors embeddings are famous for**. What are the entries most similar to \"Lady Gaga\" - \"American\" + \"Japanese\"? Table 2b) in the paper.\n",
    "\n",
    "Note that \"American\" and \"Japanese\" are word vectors, but they live in the same space as the document vectors so we can add / subtract them at will, for some interesting results. All word vectors were already lowercased by our tokenizer above, so we look for the lowercased version here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Ayumi Hamasaki', 0.6177269816398621),\n",
      " ('Dango 3 Kyodai', 0.6052653789520264),\n",
      " ('X -Cross-', 0.5981104373931885),\n",
      " ('Katy Perry', 0.5980274081230164),\n",
      " ('We Are \"Lonely Girl\"', 0.5946057438850403),\n",
      " (\"D' no Junjō\", 0.5916231274604797),\n",
      " ('Jidai (Miyuki Nakajima song)', 0.5841305255889893),\n",
      " ('Ring a Ding Dong', 0.5810612440109253),\n",
      " ('Aitakute Aitakute', 0.5777259469032288),\n",
      " ('Seventeen (South Korean band)', 0.5761528015136719)]\n",
      "Doc2Vec<dm/m,d200,n5,w8,mc5,s0.001,t20>\n",
      "[('Chisato Moritaka', 0.5491870641708374),\n",
      " ('Kaela Kimura', 0.5438317060470581),\n",
      " ('D&D (band)', 0.5341318845748901),\n",
      " ('Mari Amachi', 0.53079754114151),\n",
      " ('Rei Yasuda', 0.5302826166152954),\n",
      " ('Beautiful, Dirty, Rich', 0.5257192850112915),\n",
      " ('Radwimps', 0.5251664519309998),\n",
      " ('Pink Lady (duo)', 0.5212596654891968),\n",
      " ('Miliyah Kato', 0.5167880654335022),\n",
      " ('Koda Kumi', 0.5148667693138123)]\n"
     ]
    }
   ],
   "source": [
    "for model in [model_dbow, model_dm]:\n",
    "    print(model)\n",
    "    vec = [model.dv[\"Lady Gaga\"] - model.wv[\"american\"] + model.wv[\"japanese\"]]\n",
    "    pprint([m for m in model.dv.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the DBOW model surfaced artists similar to Lady Gaga in Japan, such as **Ayumi Hamasaki** whose Wiki bio says:\n",
    "\n",
    "> Ayumi Hamasaki is a Japanese singer, songwriter, record producer, actress, model, spokesperson, and entrepreneur.\n",
    "\n",
    "So that sounds like a success. It's also the nr. 1 hit in the paper we're replicating – success!\n",
    "\n",
    "The DM model results are opaque to me, but seem art & Japan related as well. The score deltas between these DM results are marginal, so it's likely they would change if retrained on a different version of Wikipedia. Or even when simply re-run on the same version – the doc2vec training algorithm is stochastic.\n",
    "\n",
    "These results demonstrate that both training modes employed in the original paper are outstanding for calculating similarity between document vectors, word vectors, or a combination of both. The DM mode has the added advantage of being 4x faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue your doc2vec explorations, refer to the official API documentation in Gensim: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
