{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Doc2Vec on Wikipedia articles full content\n",
    "1/13/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the **Document Embedding with Paragraph Vectors** paper, http://arxiv.org/abs/1507.07998, and it also adds on to this notebook from Gensim: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb\n",
    "\n",
    "Per Gensim: In that paper, the authors only showed results from the DBOW (\"distributed bag of words\") mode, trained on the English Wikipedia. Here we replicate this experiment using not only DBOW, but also the DM (\"distributed memory\") mode of the Paragraph Vector algorithm aka Doc2Vec.\n",
    "\n",
    "Per me: After working through this once with both DBOW and DM modes, I determined that DM mode delivered the better results when used on security log data, both in general purpose form and after I fine-tuned the base model with some security log data. The DM model mode also trains considerably faster.  I've commented out the code around the DBOW model mode, but left it in the notebook so you can try it for yourself if you choose to do so. In running the training process on both Windows and Linux hosts, Linux was close to 2X faster on the long-running processes, but they still ran on Windows.\n",
    "\n",
    "I've also uploaded a copy of the base model that is trained on the full contents of Wikipedia...you can choose to download and use or fine-tune that copy, versus building it from scratch with this notebook. See the download link below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.12 seems to have some breaking new features.  This notebook works correctly with Python 3.11 though. There's a few different ways to handle this, but I used <b>pyenv</b> as documented here: https://forums.linuxmint.com/viewtopic.php?t=362499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already present\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "from gensim.corpora.wikicorpus import WikiCorpus, tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [http://download.wikimedia.org/enwiki/latest](http://download.wikimedia.org/enwiki/latest). You want the file named `enwiki-latest-pages-articles.xml.bz2`.\n",
    "\n",
    "Second, convert that Wikipedia article dump from the Wikimedia XML format into a plain text file. This will make the subsequent training faster and also allow easy inspection of the data = \"input eyeballing\".\n",
    "\n",
    "We'll preprocess each article at the same time, normalizing its text to lowercase, splitting into tokens, etc. Below I use a regexp tokenizer that simply looks for alphabetic sequences as tokens. But feel free to adapt the text preprocessing to your own domain. High quality preprocessing is often critical for the final pipeline accuracy – garbage in, garbage out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to download the most recent Wikipedia backup/dump\n",
    "#!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:19:10,750 : INFO : processing article #0: 'Anarchism' (6790 tokens)\n",
      "2025-02-12 14:34:07,652 : INFO : processing article #500000: 'Brian Lee (wrestler)' (3057 tokens)\n",
      "2025-02-12 14:44:03,189 : INFO : processing article #1000000: 'Hay Festival' (1364 tokens)\n",
      "2025-02-12 14:52:48,948 : INFO : processing article #1500000: 'Conquered lorikeet' (128 tokens)\n",
      "2025-02-12 15:01:26,447 : INFO : processing article #2000000: 'Poverty Valley Aerodrome' (54 tokens)\n",
      "2025-02-12 15:10:07,276 : INFO : processing article #2500000: 'Get Ready (Mase song)' (136 tokens)\n",
      "2025-02-12 15:19:18,563 : INFO : processing article #3000000: 'Hans IV Jordaens' (201 tokens)\n",
      "2025-02-12 15:29:02,436 : INFO : processing article #3500000: 'Anthony Hawken' (97 tokens)\n",
      "2025-02-12 15:38:20,518 : INFO : processing article #4000000: 'The Peak Scaler' (91 tokens)\n",
      "2025-02-12 15:47:22,235 : INFO : processing article #4500000: 'Isobel Lilian Gloag' (208 tokens)\n",
      "2025-02-12 15:57:22,788 : INFO : processing article #5000000: 'Kodigehalli metro station' (259 tokens)\n",
      "2025-02-12 16:07:43,693 : INFO : processing article #5500000: 'Kirsten John-Stucke' (626 tokens)\n",
      "2025-02-12 16:11:52,081 : INFO : finished iterating over Wikipedia corpus of 5699089 documents with 3420342527 positions (total 24404020 articles, 3502376900 positions before pruning articles shorter than 50 words)\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiCorpus(\n",
    "    \"enwiki-latest-pages-articles.xml.bz2\",  # path to the file you downloaded above\n",
    "    tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n",
    "    metadata=True,  # also return the article titles and ids when parsing\n",
    "    dictionary={},  # don't start processing the data yet\n",
    ")\n",
    "\n",
    "with smart_open.open(\"training-data_wikipedia-full-content.txt.gz\", \"w\", encoding='utf8') as fout:\n",
    "    for article_no, (content, (page_id, title)) in enumerate(wiki.get_texts()):\n",
    "        title = ' '.join(title.split())\n",
    "        if article_no % 500000 == 0:\n",
    "            logging.info(\"processing article #%i: %r (%i tokens)\", article_no, title, len(content))\n",
    "        fout.write(f\"{title}\\t{' '.join(content)}\\n\")  # title_of_article [TAB] words of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took about 2 hours and created a new ~7 GB file named `training-data_wikipedia-full-content.txt.gz`. Note the output text was transparently compressed into `.gz` (GZIP) right away, using the [smart_open](https://github.com/RaRe-Technologies/smart_open) library, to save on disk space.\n",
    "\n",
    "Next we'll set up a document stream to load the preprocessed articles from the training data one by one, in the format expected by Doc2Vec, ready for training. We don't want to load everything into RAM at once, because that would blow up the memory. And it is not necessary – Gensim can handle streamed input training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiCorpus:\n",
    "    def __init__(self, wiki_text_path):\n",
    "        self.wiki_text_path = wiki_text_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in smart_open.open(self.wiki_text_path, encoding='utf8'):\n",
    "            title, words = line.split('\\t')\n",
    "            yield TaggedDocument(words=words.split(), tags=[title])\n",
    "\n",
    "documents = TaggedWikiCorpus('training-data_wikipedia-full-content.txt.gz')  # A streamed iterable; nothing in RAM yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism'] :  anarchism is political philosophy and movement that is against all forms of authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy typically including the state and capitalism anarchism advocates for the replacement of the state with stateless societies and voluntary free associations historically left wing ……… sources further reading criticism of philosophical anarchism defence of philosophical anarchism stating that both kinds of anarchism philosophical and political anarchism are philosophical and political claims anarchistic popular fiction novel an argument for philosophical anarchism external links anarchy archives an online research center on the history and theory of anarchism\n"
     ]
    }
   ],
   "source": [
    "# Load and print the first preprocessed Wikipedia document, as a sanity check = \"input eyeballing\".\n",
    "first_doc = next(iter(documents))\n",
    "print(first_doc.tags, ': ', ' '.join(first_doc.words[:50] + ['………'] + first_doc.words[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document seems legit so let's move on to finally training some Doc2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper had a vocabulary size of 915,715 word types, so we'll try to match it by setting `max_final_vocab` to 1,000,000 in the Doc2vec constructor.\n",
    "\n",
    "Other critical parameters were left unspecified in the paper, so we'll go with a window size of eight (a prediction window of 8 tokens to either side). It looks like the authors tried vector dimensionality of 100, 300, 1,000 & 10,000 in the paper (with 10k dims performing the best), but I'll only train with 200 dimensions here, to keep the RAM in check on my laptop.\n",
    "\n",
    "Feel free to tinker with these values yourself if you like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:15:43,317 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d256,n5,w8,mc5,s0.001,t12>', 'datetime': '2025-02-12T16:15:43.316999', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "workers = 12  # multiprocessing.cpu_count() - 1  # leave one core for the OS & other stuff\n",
    "\n",
    "# PV-DBOW: paragraph vector in distributed bag of words mode\n",
    "#model_dbow = Doc2Vec(\n",
    "#    dm=0, dbow_words=1,  # dbow_words=1 to train word vectors at the same time too, not only DBOW\n",
    "#    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    "#)\n",
    "\n",
    "# PV-DM: paragraph vector in distributed memory mode\n",
    "model_dm = Doc2Vec(\n",
    "    dm=1, dm_mean=1,  # use average of context word vectors to train DM\n",
    "    vector_size=256, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one pass through the Wikipedia corpus, to collect the 1M vocabulary and initialize the doc2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:17:20,953 : INFO : collecting all words and their counts\n",
      "2025-02-12 16:17:20,956 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-02-12 16:20:04,662 : INFO : PROGRESS: at example #500000, processed 691945216 words (4226775 words/s), 3302056 word types, 500000 tags\n",
      "2025-02-12 16:21:39,356 : INFO : PROGRESS: at example #1000000, processed 1076732994 words (4063659 words/s), 4589545 word types, 1000000 tags\n",
      "2025-02-12 16:22:55,513 : INFO : PROGRESS: at example #1500000, processed 1378006617 words (3955982 words/s), 5550052 word types, 1500000 tags\n",
      "2025-02-12 16:24:01,459 : INFO : PROGRESS: at example #2000000, processed 1635595979 words (3906086 words/s), 6332484 word types, 2000000 tags\n",
      "2025-02-12 16:25:05,278 : INFO : PROGRESS: at example #2500000, processed 1887726556 words (3950759 words/s), 7100806 word types, 2500000 tags\n",
      "2025-02-12 16:26:08,999 : INFO : PROGRESS: at example #3000000, processed 2136847999 words (3909570 words/s), 7843136 word types, 3000000 tags\n",
      "2025-02-12 16:27:11,988 : INFO : PROGRESS: at example #3500000, processed 2381863205 words (3889887 words/s), 8529199 word types, 3500000 tags\n",
      "2025-02-12 16:28:12,065 : INFO : PROGRESS: at example #4000000, processed 2617434715 words (3921185 words/s), 9154306 word types, 4000000 tags\n",
      "2025-02-12 16:29:11,963 : INFO : PROGRESS: at example #4500000, processed 2845388955 words (3805691 words/s), 9772828 word types, 4500000 tags\n",
      "2025-02-12 16:30:12,836 : INFO : PROGRESS: at example #5000000, processed 3084175169 words (3922747 words/s), 10403377 word types, 5000000 tags\n",
      "2025-02-12 16:31:16,797 : INFO : PROGRESS: at example #5500000, processed 3325646743 words (3775343 words/s), 11014173 word types, 5500000 tags\n",
      "2025-02-12 16:32:03,957 : INFO : collected 11239083 word types and 5699089 unique tags from a corpus of 5699089 examples and 3420342527 words\n",
      "2025-02-12 16:32:07,730 : INFO : Doc2Vec lifecycle event {'msg': 'max_final_vocab=1000000 and min_count=5 resulted in calc_min_count=27, effective_min_count=27', 'datetime': '2025-02-12T16:32:07.730369', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-12 16:32:07,730 : INFO : Creating a fresh vocabulary\n",
      "2025-02-12 16:32:11,820 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 retains 981089 unique words (8.73% of original 11239083, drops 10257994)', 'datetime': '2025-02-12T16:32:11.820595', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-12 16:32:11,821 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 leaves 3387835608 word corpus (99.05% of original 3420342527, drops 32506919)', 'datetime': '2025-02-12T16:32:11.821287', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-12 16:32:15,918 : INFO : deleting the raw counts dictionary of 11239083 items\n",
      "2025-02-12 16:32:16,050 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2025-02-12 16:32:16,051 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2756179411.136269 word corpus (81.4%% of prior 3387835608)', 'datetime': '2025-02-12T16:32:16.051463', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-12 16:32:22,476 : INFO : estimated required memory for 981089 words and 256 dimensions: 9475499708 bytes\n",
      "2025-02-12 16:32:22,477 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d256,n5,w8,mc5,s0.001,t12>\n"
     ]
    }
   ],
   "source": [
    "#model_dbow.build_vocab(documents, progress_per=500000)\n",
    "#print(model_dbow)\n",
    "\n",
    "# Save some time by copying the vocabulary structures from the DBOW model to the DM model.\n",
    "# Both models are built on top of exactly the same data, so there's no need to repeat the vocab-building step.\n",
    "#model_dm.reset_from(model_dbow)\n",
    "model_dm.build_vocab(documents, progress_per=500000)\n",
    "print(model_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to train Doc2Vec on the entirety of the English Wikipedia. **Warning!** Training these models can take 6-18 hours depending on your compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train DBOW doc2vec incl. word vectors.\n",
    "# Report progress every ½ hour.\n",
    "# NOTE: This ran for ~20 hours on a Windows 10 laptop with 12 cores, 128Gb ram, and 1Tb SSD\n",
    "#model_dbow.train(documents, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=30*60)\n",
    "#model_dbow.save('doc2vec_wikipedia_dbow.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 16:34:49,618 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 12 workers on 981089 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2025-02-12T16:34:49.618448', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2025-02-12 16:34:50,624 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 921087 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 17:04:50,638 : INFO : EPOCH 0 - PROGRESS: at 52.14% examples, 943748 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 17:25:38,921 : INFO : EPOCH 0: training on 3420342527 raw words (2743634368 effective words) took 3049.3s, 899759 effective words/s\n",
      "2025-02-12 17:25:39,929 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 1158951 words/s, in_qsize 0, out_qsize 1\n",
      "2025-02-12 17:55:39,935 : INFO : EPOCH 1 - PROGRESS: at 52.68% examples, 950137 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 18:16:08,510 : INFO : EPOCH 1: training on 3420342527 raw words (2743632300 effective words) took 3029.6s, 905613 effective words/s\n",
      "2025-02-12 18:16:09,515 : INFO : EPOCH 2 - PROGRESS: at 0.01% examples, 1218586 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 18:46:09,516 : INFO : EPOCH 2 - PROGRESS: at 52.85% examples, 952134 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 19:06:31,438 : INFO : EPOCH 2: training on 3420342527 raw words (2743663025 effective words) took 3022.9s, 907618 effective words/s\n",
      "2025-02-12 19:06:32,444 : INFO : EPOCH 3 - PROGRESS: at 0.01% examples, 1199726 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 19:36:32,448 : INFO : EPOCH 3 - PROGRESS: at 52.90% examples, 952818 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 19:56:53,110 : INFO : EPOCH 3: training on 3420342527 raw words (2743645270 effective words) took 3021.7s, 907991 effective words/s\n",
      "2025-02-12 19:56:54,114 : INFO : EPOCH 4 - PROGRESS: at 0.01% examples, 1233627 words/s, in_qsize 0, out_qsize 1\n",
      "2025-02-12 20:26:54,125 : INFO : EPOCH 4 - PROGRESS: at 52.95% examples, 953274 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 20:47:12,209 : INFO : EPOCH 4: training on 3420342527 raw words (2743628871 effective words) took 3019.1s, 908758 effective words/s\n",
      "2025-02-12 20:47:13,218 : INFO : EPOCH 5 - PROGRESS: at 0.01% examples, 1222377 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 21:17:13,239 : INFO : EPOCH 5 - PROGRESS: at 52.94% examples, 953211 words/s, in_qsize 24, out_qsize 0\n",
      "2025-02-12 21:37:40,830 : INFO : EPOCH 5: training on 3420342527 raw words (2743644192 effective words) took 3028.6s, 905906 effective words/s\n",
      "2025-02-12 21:37:41,841 : INFO : EPOCH 6 - PROGRESS: at 0.01% examples, 1212066 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 22:07:41,859 : INFO : EPOCH 6 - PROGRESS: at 52.52% examples, 948366 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-12 22:28:17,218 : INFO : EPOCH 6: training on 3420342527 raw words (2743645461 effective words) took 3036.4s, 903590 effective words/s\n",
      "2025-02-12 22:28:18,222 : INFO : EPOCH 7 - PROGRESS: at 0.01% examples, 1224282 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 22:58:18,223 : INFO : EPOCH 7 - PROGRESS: at 52.65% examples, 949827 words/s, in_qsize 24, out_qsize 0\n",
      "2025-02-12 23:18:50,339 : INFO : EPOCH 7: training on 3420342527 raw words (2743635190 effective words) took 3033.1s, 904559 effective words/s\n",
      "2025-02-12 23:18:51,346 : INFO : EPOCH 8 - PROGRESS: at 0.01% examples, 1198284 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-12 23:48:51,357 : INFO : EPOCH 8 - PROGRESS: at 52.67% examples, 950090 words/s, in_qsize 20, out_qsize 0\n",
      "2025-02-13 00:09:23,017 : INFO : EPOCH 8: training on 3420342527 raw words (2743630167 effective words) took 3032.7s, 904690 effective words/s\n",
      "2025-02-13 00:09:24,022 : INFO : EPOCH 9 - PROGRESS: at 0.01% examples, 1182775 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 00:39:24,026 : INFO : EPOCH 9 - PROGRESS: at 52.73% examples, 950749 words/s, in_qsize 24, out_qsize 0\n",
      "2025-02-13 00:59:54,118 : INFO : EPOCH 9: training on 3420342527 raw words (2743635316 effective words) took 3031.1s, 905162 effective words/s\n",
      "2025-02-13 00:59:54,118 : INFO : Doc2Vec lifecycle event {'msg': 'training on 34203425270 raw words (27436394160 effective words) took 30304.5s, 905357 effective words/s', 'datetime': '2025-02-13T00:59:54.118888', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2025-02-13 00:59:54,119 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_wikipedia_dm.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-02-13T00:59:54.119386', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "2025-02-13 00:59:54,119 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.dv.vectors.npy\n",
      "2025-02-13 00:59:55,912 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.wv.vectors.npy\n",
      "2025-02-13 00:59:56,227 : INFO : storing np array 'syn1neg' to doc2vec_wikipedia_dm.model.syn1neg.npy\n",
      "2025-02-13 00:59:56,548 : INFO : not storing attribute cum_table\n",
      "2025-02-13 00:59:59,553 : INFO : saved doc2vec_wikipedia_dm.model\n"
     ]
    }
   ],
   "source": [
    "# Train DM doc2vec.\n",
    "# NOTE: This ran for ~8.5 hours on Intel NUC w/ 16 cores, 64Gb ram, 1Tb SDD, and Linux Mint 22\n",
    "model_dm.train(documents, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=30*60)\n",
    "model_dm.save('doc2vec_wikipedia_dm.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models have been saved to the url's below so you can avoid this 1 day of training compute time if you are just looking to experiment. The model is around 7Gb in size.\n",
    "- doc2vec_wikipedia_dm-model.256.20250212.zip (256 dimension, wikipedia data as of 2025-02-12): \n",
    "https://mega.nz/file/m6ICnQxb#tUY8hCGhScyAOf3Y7HONNk7GsGrftcpYNFLZw2QZHrU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already trained or downloaded/unzipped the models and you are picking up here, run the first cell with the imports and then load the models below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dbow = Doc2Vec.load('doc2vec_wikipedia_dbow.model')\n",
    "model_dm = Doc2Vec.load('doc2vec_wikipedia_dm.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the most similar Wikipedia articles to the \"Machine learning\" article. The calculated word vectors and document vectors are stored separately, in `model.wv` and `model.dv` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d256,n5,w8,mc5,s0.001,t12>\n",
      "[('Pattern recognition', 0.6913762092590332),\n",
      " ('Supervised learning', 0.6727138757705688),\n",
      " ('Neural network (machine learning)', 0.6502088904380798),\n",
      " ('Meta-learning (computer science)', 0.6317244172096252),\n",
      " ('Feature learning', 0.6298242807388306),\n",
      " ('Anomaly detection', 0.6272202730178833),\n",
      " ('Feature selection', 0.6252041459083557),\n",
      " ('Linear classifier', 0.616140604019165),\n",
      " ('Ensemble learning', 0.615498423576355),\n",
      " ('Boosting (machine learning)', 0.6137340664863586),\n",
      " ('Naive Bayes classifier', 0.610593855381012),\n",
      " ('Automatic image annotation', 0.6056917309761047),\n",
      " ('Multiclass classification', 0.6055539846420288),\n",
      " ('Multi-task learning', 0.6039445400238037),\n",
      " ('Statistical classification', 0.603486955165863),\n",
      " ('Regularization (mathematics)', 0.6019155979156494),\n",
      " ('Artificial intelligence', 0.6012045741081238),\n",
      " ('Random subspace method', 0.5989224314689636),\n",
      " ('Early stopping', 0.5979453325271606),\n",
      " ('Latent space', 0.5958713293075562)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d256,n5,w8,mc5,s0.001,t12>\n",
      "[('Soundgarden', 0.6760900020599365),\n",
      " ('Temple of the Dog', 0.6714499592781067),\n",
      " ('Alice in Chains', 0.6464263200759888),\n",
      " ('Scott Weiland', 0.6344491839408875),\n",
      " ('Layne Staley', 0.6146446466445923),\n",
      " ('Audioslave', 0.6069024205207825),\n",
      " ('Chester Bennington', 0.6001834869384766),\n",
      " ('Euphoria Morning', 0.5890706777572632),\n",
      " ('Louder Than Love', 0.5855295062065125),\n",
      " ('Hunger Strike (song)', 0.5804468989372253)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Chris Cornell\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'll keep the commentary from the original notebook...my search term was 'Chris Cornell' instead of 'Lady Gaga' though: \n",
    "The DBOW results are in line with what the paper shows in Table 2a), revealing similar singers in the U.S. Interestingly, the DM results seem to capture more \"fact about Lady Gaga\" (her albums, trivia), whereas DBOW recovered \"similar artists\".\n",
    "\n",
    "**Finally, let's do some of the wilder arithmetics that vectors embeddings are famous for**. What are the entries most similar to \"Lady Gaga\" - \"American\" + \"Japanese\"? Table 2b) in the paper.\n",
    "Note that \"American\" and \"Japanese\" are word vectors, but they live in the same space as the document vectors so we can add / subtract them at will, for some interesting results. All word vectors were already lowercased by our tokenizer above, so we look for the lowercased version here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d256,n5,w8,mc5,s0.001,t12>\n",
      "[('Jennifer Lopez', 0.5050081014633179),\n",
      " ('Born This Way (album)', 0.49613016843795776),\n",
      " ('Taylor Swift', 0.4820432662963867),\n",
      " ('Lady Gaga videography', 0.4794251024723053),\n",
      " ('Bad Romance', 0.4734669327735901),\n",
      " ('Selena Gomez', 0.4711049199104309),\n",
      " ('Adele', 0.46933799982070923),\n",
      " ('Lizzo', 0.46371859312057495),\n",
      " ('Normani', 0.4565371870994568),\n",
      " ('Beyoncé', 0.4494146406650543)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    vec = [model.dv[\"Lady Gaga\"] + model.wv[\"american\"] - model.wv[\"japanese\"]]\n",
    "    # I switched the search math here...+ american and - japanese\n",
    "    pprint([m for m in model.dv.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results demonstrate that both training modes employed in the original paper are outstanding for calculating similarity between document vectors, word vectors, or a combination of both. The DM mode has the added advantage of being 4x faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue your doc2vec explorations, refer to the official API documentation in Gensim: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
