{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Doc2Vec on Wikipedia articles full content\n",
    "1/13/2025, Dave Sisk, https://github.com/davidcsisk, https://www.linkedin.com/in/davesisk-doctordatabase/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the **Document Embedding with Paragraph Vectors** paper, http://arxiv.org/abs/1507.07998, and it also adds on to this notebook from Gensim: https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-wikipedia.ipynb\n",
    "\n",
    "After working through this once with both DBOW and DM modes, I determined that DM mode delivered the better results when used on security log data, both in general purpose form and after I fine-tuned the base model with security log data. The DM model mode also trains considerably faster.  I've commented out the code around the DBOW model mode, but left it in the notebook so you can try it for yourself if you choose to do so. In running the training process on both Windows and Linux hosts, Linux was close to 2X faster on the long-running processes, but they still ran to completion on Windows. I've built the model with 200 dimensions as was originally done, plus with 256 and 512 dimensions. The training time does not get substantially longer, but the model size and memory requirements do get larger...when you double the dimensions of the model you double the size of the model. Note that the \"build vocabulary\" step below as one of it's outputs gives an estimate of the available memory required to train the model...it was around 10Gb required for 1M words & 256 dimensions, and 18Gb for 1M words & 512 dimensions.\n",
    "\n",
    "I've uploaded a copy of the 256 dimension base model trained on the full contents of Wikipedia...you can choose to download and use or fine-tune that copy, versus building it from scratch with this notebook. You can find that here: https://mega.nz/file/m6ICnQxb#tUY8hCGhScyAOf3Y7HONNk7GsGrftcpYNFLZw2QZHrU \n",
    "\n",
    "I also have an example of fine-tuning this doc2vec model with domain-specific data content...you can find that here: https://github.com/davidcsisk/vector-search-with-security-logs/blob/main/fine-tune-doc2vec-model_dns-logs.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.12 seems to have some breaking new features.  This notebook works correctly with Python 3.11 though. There's a few different ways to handle this, but I used <b>pyenv</b> as documented here: https://forums.linuxmint.com/viewtopic.php?t=362499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if not already present\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "import smart_open\n",
    "from gensim.corpora.wikicorpus import WikiCorpus, tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dump of all Wikipedia articles from [http://download.wikimedia.org/enwiki/latest](http://download.wikimedia.org/enwiki/latest). You want the file named `enwiki-latest-pages-articles.xml.bz2`.\n",
    "\n",
    "Second, convert that Wikipedia article dump from the Wikimedia XML format into a plain text file. This will make the subsequent training faster and also allow easy inspection of the data = \"input eyeballing\".\n",
    "\n",
    "We'll preprocess each article at the same time, normalizing its text to lowercase, splitting into tokens, etc. Below I use a regexp tokenizer that simply looks for alphabetic sequences as tokens. But feel free to adapt the text preprocessing to your own domain. High quality preprocessing is often critical for the final pipeline accuracy – garbage in, garbage out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to download the most recent Wikipedia backup/dump\n",
    "#!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 14:19:10,750 : INFO : processing article #0: 'Anarchism' (6790 tokens)\n",
      "2025-02-12 14:34:07,652 : INFO : processing article #500000: 'Brian Lee (wrestler)' (3057 tokens)\n",
      "2025-02-12 14:44:03,189 : INFO : processing article #1000000: 'Hay Festival' (1364 tokens)\n",
      "2025-02-12 14:52:48,948 : INFO : processing article #1500000: 'Conquered lorikeet' (128 tokens)\n",
      "2025-02-12 15:01:26,447 : INFO : processing article #2000000: 'Poverty Valley Aerodrome' (54 tokens)\n",
      "2025-02-12 15:10:07,276 : INFO : processing article #2500000: 'Get Ready (Mase song)' (136 tokens)\n",
      "2025-02-12 15:19:18,563 : INFO : processing article #3000000: 'Hans IV Jordaens' (201 tokens)\n",
      "2025-02-12 15:29:02,436 : INFO : processing article #3500000: 'Anthony Hawken' (97 tokens)\n",
      "2025-02-12 15:38:20,518 : INFO : processing article #4000000: 'The Peak Scaler' (91 tokens)\n",
      "2025-02-12 15:47:22,235 : INFO : processing article #4500000: 'Isobel Lilian Gloag' (208 tokens)\n",
      "2025-02-12 15:57:22,788 : INFO : processing article #5000000: 'Kodigehalli metro station' (259 tokens)\n",
      "2025-02-12 16:07:43,693 : INFO : processing article #5500000: 'Kirsten John-Stucke' (626 tokens)\n",
      "2025-02-12 16:11:52,081 : INFO : finished iterating over Wikipedia corpus of 5699089 documents with 3420342527 positions (total 24404020 articles, 3502376900 positions before pruning articles shorter than 50 words)\n"
     ]
    }
   ],
   "source": [
    "wiki = WikiCorpus(\n",
    "    \"enwiki-latest-pages-articles.xml.bz2\",  # path to the file you downloaded above\n",
    "    tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n",
    "    metadata=True,  # also return the article titles and ids when parsing\n",
    "    dictionary={},  # don't start processing the data yet\n",
    ")\n",
    "\n",
    "with smart_open.open(\"training-data_wikipedia-full-content.txt.gz\", \"w\", encoding='utf8') as fout:\n",
    "    for article_no, (content, (page_id, title)) in enumerate(wiki.get_texts()):\n",
    "        title = ' '.join(title.split())\n",
    "        if article_no % 500000 == 0:\n",
    "            logging.info(\"processing article #%i: %r (%i tokens)\", article_no, title, len(content))\n",
    "        fout.write(f\"{title}\\t{' '.join(content)}\\n\")  # title_of_article [TAB] words of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took about 2 hours and created a new ~7 GB file named `training-data_wikipedia-full-content.txt.gz`. Note the output text was transparently compressed into `.gz` (GZIP) right away, using the [smart_open](https://github.com/RaRe-Technologies/smart_open) library, to save on disk space.\n",
    "\n",
    "Next we'll set up a document stream to load the preprocessed articles from the training data one by one, in the format expected by Doc2Vec, ready for training. We don't want to load everything into RAM at once, because that would blow up the memory. And it is not necessary – Gensim can handle streamed input training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedWikiCorpus:\n",
    "    def __init__(self, wiki_text_path):\n",
    "        self.wiki_text_path = wiki_text_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in smart_open.open(self.wiki_text_path, encoding='utf8'):\n",
    "            title, words = line.split('\\t')\n",
    "            yield TaggedDocument(words=words.split(), tags=[title])\n",
    "\n",
    "documents = TaggedWikiCorpus('training-data_wikipedia-full-content.txt.gz')  # A streamed iterable; nothing in RAM yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism'] :  anarchism is political philosophy and movement that is against all forms of authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy typically including the state and capitalism anarchism advocates for the replacement of the state with stateless societies and voluntary free associations historically left wing ……… sources further reading criticism of philosophical anarchism defence of philosophical anarchism stating that both kinds of anarchism philosophical and political anarchism are philosophical and political claims anarchistic popular fiction novel an argument for philosophical anarchism external links anarchy archives an online research center on the history and theory of anarchism\n"
     ]
    }
   ],
   "source": [
    "# Load and print the first preprocessed Wikipedia document, as a sanity check = \"input eyeballing\".\n",
    "first_doc = next(iter(documents))\n",
    "print(first_doc.tags, ': ', ' '.join(first_doc.words[:50] + ['………'] + first_doc.words[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document seems legit so let's move on to finally training some Doc2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper had a vocabulary size of 915,715 word types, so we'll try to match it by setting `max_final_vocab` to 1,000,000 in the Doc2vec constructor.\n",
    "\n",
    "Other critical parameters were left unspecified in the paper, so we'll go with a window size of eight (a prediction window of 8 tokens to either side). It looks like the authors tried vector dimensionality of 100, 300, 1,000 & 10,000 in the paper (with 10k dims performing the best), but I'll only train with 200 dimensions here, to keep the RAM in check on my laptop.  UPDATE:  This notebook reflects 256 and 512 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:05:44,177 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d512,n5,w8,mc5,s0.001,t12>', 'datetime': '2025-02-13T16:05:44.177896', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "workers = 12  # multiprocessing.cpu_count() - 1  # leave one core for the OS & other stuff\n",
    "\n",
    "# PV-DBOW: paragraph vector in distributed bag of words mode\n",
    "#model_dbow = Doc2Vec(\n",
    "#    dm=0, dbow_words=1,  # dbow_words=1 to train word vectors at the same time too, not only DBOW\n",
    "#    vector_size=200, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    "#)\n",
    "\n",
    "# PV-DM: paragraph vector in distributed memory mode\n",
    "model_dm = Doc2Vec(\n",
    "    dm=1, dm_mean=1,  # use average of context word vectors to train DM\n",
    "    #vector_size=256, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    "    vector_size=512, window=8, epochs=10, workers=workers, max_final_vocab=1000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one pass through the Wikipedia corpus, to collect the 1M vocabulary and initialize the doc2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:06:01,137 : INFO : collecting all words and their counts\n",
      "2025-02-13 16:06:01,140 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-02-13 16:08:26,382 : INFO : PROGRESS: at example #500000, processed 691945216 words (4764086 words/s), 3302056 word types, 500000 tags\n",
      "2025-02-13 16:09:50,454 : INFO : PROGRESS: at example #1000000, processed 1076732994 words (4576897 words/s), 4589545 word types, 1000000 tags\n",
      "2025-02-13 16:10:58,534 : INFO : PROGRESS: at example #1500000, processed 1378006617 words (4425354 words/s), 5550052 word types, 1500000 tags\n",
      "2025-02-13 16:11:56,887 : INFO : PROGRESS: at example #2000000, processed 1635595979 words (4414353 words/s), 6332484 word types, 2000000 tags\n",
      "2025-02-13 16:12:54,184 : INFO : PROGRESS: at example #2500000, processed 1887726556 words (4400441 words/s), 7100806 word types, 2500000 tags\n",
      "2025-02-13 16:13:51,483 : INFO : PROGRESS: at example #3000000, processed 2136847999 words (4347766 words/s), 7843136 word types, 3000000 tags\n",
      "2025-02-13 16:14:48,306 : INFO : PROGRESS: at example #3500000, processed 2381863205 words (4311989 words/s), 8529199 word types, 3500000 tags\n",
      "2025-02-13 16:15:42,246 : INFO : PROGRESS: at example #4000000, processed 2617434715 words (4367311 words/s), 9154306 word types, 4000000 tags\n",
      "2025-02-13 16:16:35,989 : INFO : PROGRESS: at example #4500000, processed 2845388955 words (4241629 words/s), 9772828 word types, 4500000 tags\n",
      "2025-02-13 16:17:30,905 : INFO : PROGRESS: at example #5000000, processed 3084175169 words (4348248 words/s), 10403377 word types, 5000000 tags\n",
      "2025-02-13 16:18:28,492 : INFO : PROGRESS: at example #5500000, processed 3325646743 words (4193159 words/s), 11014173 word types, 5500000 tags\n",
      "2025-02-13 16:19:12,070 : INFO : collected 11239083 word types and 5699089 unique tags from a corpus of 5699089 examples and 3420342527 words\n",
      "2025-02-13 16:19:15,612 : INFO : Doc2Vec lifecycle event {'msg': 'max_final_vocab=1000000 and min_count=5 resulted in calc_min_count=27, effective_min_count=27', 'datetime': '2025-02-13T16:19:15.612463', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-13 16:19:15,612 : INFO : Creating a fresh vocabulary\n",
      "2025-02-13 16:19:19,379 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 retains 981089 unique words (8.73% of original 11239083, drops 10257994)', 'datetime': '2025-02-13T16:19:19.379160', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-13 16:19:19,379 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=27 leaves 3387835608 word corpus (99.05% of original 3420342527, drops 32506919)', 'datetime': '2025-02-13T16:19:19.379673', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-13 16:19:23,127 : INFO : deleting the raw counts dictionary of 11239083 items\n",
      "2025-02-13 16:19:23,244 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2025-02-13 16:19:23,245 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 2756179411.136269 word corpus (81.4%% of prior 3387835608)', 'datetime': '2025-02-13T16:19:23.245866', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2025-02-13 16:19:29,450 : INFO : estimated required memory for 981089 words and 512 dimensions: 17320637116 bytes\n",
      "2025-02-13 16:19:29,450 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d512,n5,w8,mc5,s0.001,t12>\n"
     ]
    }
   ],
   "source": [
    "#model_dbow.build_vocab(documents, progress_per=500000)\n",
    "#print(model_dbow)\n",
    "\n",
    "# Save some time by copying the vocabulary structures from the DBOW model to the DM model.\n",
    "# Both models are built on top of exactly the same data, so there's no need to repeat the vocab-building step.\n",
    "#model_dm.reset_from(model_dbow)\n",
    "model_dm.build_vocab(documents, progress_per=500000)\n",
    "print(model_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to train Doc2Vec on the entirety of the English Wikipedia. **Warning!** Training these models can take 6-18 hours depending on your compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train DBOW doc2vec incl. word vectors.\n",
    "# Report progress every ½ hour.\n",
    "# NOTE: This ran for ~20 hours on a Windows 10 laptop with 12 cores, 128Gb ram, and 1Tb SSD\n",
    "#model_dbow.train(documents, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=30*60)\n",
    "#model_dbow.save('doc2vec_wikipedia_dbow.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 16:22:01,348 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 12 workers on 981089 vocabulary and 512 features, using sg=0 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2025-02-13T16:22:01.348410', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2025-02-13 16:22:02,356 : INFO : EPOCH 0 - PROGRESS: at 0.00% examples, 737455 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 16:52:02,361 : INFO : EPOCH 0 - PROGRESS: at 50.00% examples, 914589 words/s, in_qsize 19, out_qsize 0\n",
      "2025-02-13 17:13:41,344 : INFO : EPOCH 0: training on 3420342527 raw words (2743627808 effective words) took 3100.0s, 885044 effective words/s\n",
      "2025-02-13 17:13:42,365 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 1053787 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 17:43:42,366 : INFO : EPOCH 1 - PROGRESS: at 51.84% examples, 939950 words/s, in_qsize 24, out_qsize 0\n",
      "2025-02-13 18:04:16,807 : INFO : EPOCH 1: training on 3420342527 raw words (2743651040 effective words) took 3035.5s, 903867 effective words/s\n",
      "2025-02-13 18:04:17,816 : INFO : EPOCH 2 - PROGRESS: at 0.01% examples, 1088625 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 18:34:17,833 : INFO : EPOCH 2 - PROGRESS: at 52.19% examples, 944301 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-13 18:54:26,573 : INFO : EPOCH 2: training on 3420342527 raw words (2743634998 effective words) took 3009.8s, 911578 effective words/s\n",
      "2025-02-13 18:54:27,582 : INFO : EPOCH 3 - PROGRESS: at 0.01% examples, 1151212 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 19:24:27,582 : INFO : EPOCH 3 - PROGRESS: at 52.80% examples, 951635 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-13 19:44:18,979 : INFO : EPOCH 3: training on 3420342527 raw words (2743624814 effective words) took 2992.4s, 916864 effective words/s\n",
      "2025-02-13 19:44:19,995 : INFO : EPOCH 4 - PROGRESS: at 0.01% examples, 1142519 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 20:14:20,000 : INFO : EPOCH 4 - PROGRESS: at 52.86% examples, 952267 words/s, in_qsize 18, out_qsize 0\n",
      "2025-02-13 20:34:09,880 : INFO : EPOCH 4: training on 3420342527 raw words (2743633765 effective words) took 2990.9s, 917328 effective words/s\n",
      "2025-02-13 20:34:10,889 : INFO : EPOCH 5 - PROGRESS: at 0.01% examples, 1114277 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 21:04:10,891 : INFO : EPOCH 5 - PROGRESS: at 53.00% examples, 953846 words/s, in_qsize 19, out_qsize 0\n",
      "2025-02-13 21:23:56,167 : INFO : EPOCH 5: training on 3420342527 raw words (2743639656 effective words) took 2986.3s, 918747 effective words/s\n",
      "2025-02-13 21:23:57,175 : INFO : EPOCH 6 - PROGRESS: at 0.01% examples, 1151834 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 21:53:57,184 : INFO : EPOCH 6 - PROGRESS: at 53.05% examples, 954453 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-13 22:13:40,522 : INFO : EPOCH 6: training on 3420342527 raw words (2743652856 effective words) took 2984.4s, 919346 effective words/s\n",
      "2025-02-13 22:13:41,526 : INFO : EPOCH 7 - PROGRESS: at 0.01% examples, 1148082 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 22:43:41,537 : INFO : EPOCH 7 - PROGRESS: at 53.07% examples, 954684 words/s, in_qsize 22, out_qsize 0\n",
      "2025-02-13 23:03:35,111 : INFO : EPOCH 7: training on 3420342527 raw words (2743633810 effective words) took 2994.6s, 916198 effective words/s\n",
      "2025-02-13 23:03:36,117 : INFO : EPOCH 8 - PROGRESS: at 0.01% examples, 1146283 words/s, in_qsize 0, out_qsize 0\n",
      "2025-02-13 23:33:36,134 : INFO : EPOCH 8 - PROGRESS: at 52.54% examples, 948561 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-13 23:53:46,012 : INFO : EPOCH 8: training on 3420342527 raw words (2743659761 effective words) took 3010.9s, 911243 effective words/s\n",
      "2025-02-13 23:53:47,021 : INFO : EPOCH 9 - PROGRESS: at 0.01% examples, 1119850 words/s, in_qsize 1, out_qsize 0\n",
      "2025-02-14 00:23:47,024 : INFO : EPOCH 9 - PROGRESS: at 52.87% examples, 952333 words/s, in_qsize 23, out_qsize 0\n",
      "2025-02-14 00:43:35,383 : INFO : EPOCH 9: training on 3420342527 raw words (2743651816 effective words) took 2989.4s, 917803 effective words/s\n",
      "2025-02-14 00:43:35,384 : INFO : Doc2Vec lifecycle event {'msg': 'training on 34203425270 raw words (27436410324 effective words) took 30094.0s, 911689 effective words/s', 'datetime': '2025-02-14T00:43:35.384435', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2025-02-14 00:43:35,384 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_wikipedia_dm.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-02-14T00:43:35.384950', 'gensim': '4.3.3', 'python': '3.11.11 (main, Feb 12 2025, 14:14:40) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-51-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "2025-02-14 00:43:35,385 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.dv.vectors.npy\n",
      "2025-02-14 00:43:39,660 : INFO : storing np array 'vectors' to doc2vec_wikipedia_dm.model.wv.vectors.npy\n",
      "2025-02-14 00:43:40,889 : INFO : storing np array 'syn1neg' to doc2vec_wikipedia_dm.model.syn1neg.npy\n",
      "2025-02-14 00:43:42,114 : INFO : not storing attribute cum_table\n",
      "2025-02-14 00:43:44,902 : INFO : saved doc2vec_wikipedia_dm.model\n"
     ]
    }
   ],
   "source": [
    "# Train DM doc2vec.\n",
    "# NOTE: This ran for ~8.5 hours on Intel NUC w/ 16 cores, 64Gb ram, 1Tb SDD, and Linux Mint 22\n",
    "model_dm.train(documents, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=30*60)\n",
    "model_dm.save('doc2vec_wikipedia_dm.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already trained or downloaded/unzipped the models and you are picking up here, run the first cell with the imports and then load the models below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dbow = Doc2Vec.load('doc2vec_wikipedia_dbow.model')\n",
    "model_dm = Doc2Vec.load('doc2vec_wikipedia_dm.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the most similar Wikipedia articles to the \"Machine learning\" article. The calculated word vectors and document vectors are stored separately, in `model.wv` and `model.dv` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d512,n5,w8,mc5,s0.001,t12>\n",
      "[('Supervised learning', 0.6002721786499023),\n",
      " ('Pattern recognition', 0.5884252786636353),\n",
      " ('Neural network (machine learning)', 0.5640464425086975),\n",
      " ('Feature learning', 0.5582245588302612),\n",
      " ('Boosting (machine learning)', 0.5408506989479065),\n",
      " ('Meta-learning (computer science)', 0.5337782502174377),\n",
      " ('Support vector machine', 0.5289270281791687),\n",
      " ('Self-supervised learning', 0.5228670239448547),\n",
      " ('Deep learning', 0.5220445394515991),\n",
      " ('Statistical learning theory', 0.5205954313278198),\n",
      " ('Early stopping', 0.5173967480659485),\n",
      " ('Statistical classification', 0.5170636773109436),\n",
      " ('Kernel method', 0.5164067149162292),\n",
      " ('Latent space', 0.5162718296051025),\n",
      " ('Multiclass classification', 0.5132991671562195),\n",
      " ('Anomaly detection', 0.5114651322364807),\n",
      " ('Ensemble learning', 0.5109619498252869),\n",
      " ('Types of artificial neural networks', 0.506924033164978),\n",
      " ('Dimensionality reduction', 0.5068263411521912),\n",
      " ('Automatic image annotation', 0.5065488219261169)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Machine learning\"], topn=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d512,n5,w8,mc5,s0.001,t12>\n",
      "[('Soundgarden', 0.6114035844802856),\n",
      " ('Temple of the Dog', 0.6049060821533203),\n",
      " ('Audioslave', 0.5318444967269897),\n",
      " ('Chester Bennington', 0.5158218145370483),\n",
      " ('Euphoria Morning', 0.5105340480804443),\n",
      " ('List of songs recorded by Chris Cornell', 0.5040150284767151),\n",
      " ('Chris Cornell (album)', 0.5024819374084473),\n",
      " ('Hunger Strike (song)', 0.5020096302032471),\n",
      " ('Scott Weiland', 0.4976997971534729),\n",
      " ('Layne Staley', 0.49026787281036377)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    pprint(model.dv.most_similar(positive=[\"Chris Cornell\"], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'll keep the commentary from the original notebook...my search term was 'Chris Cornell' instead of 'Lady Gaga' though: \n",
    "The DBOW results are in line with what the paper shows in Table 2a), revealing similar singers in the U.S. Interestingly, the DM results seem to capture more \"fact about Lady Gaga\" (her albums, trivia), whereas DBOW recovered \"similar artists\".\n",
    "\n",
    "**Finally, let's do some of the wilder arithmetics that vectors embeddings are famous for**. What are the entries most similar to \"Lady Gaga\" - \"American\" + \"Japanese\"? Table 2b) in the paper.\n",
    "Note that \"American\" and \"Japanese\" are word vectors, but they live in the same space as the document vectors so we can add / subtract them at will, for some interesting results. All word vectors were already lowercased by our tokenizer above, so we look for the lowercased version here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dm/m,d512,n5,w8,mc5,s0.001,t12>\n",
      "[('Lady Gaga discography', 0.464813768863678),\n",
      " ('List of awards and nominations received by Lady Gaga', 0.42341378331184387),\n",
      " ('Taylor Swift', 0.39148232340812683),\n",
      " ('Born This Way (album)', 0.3899199068546295),\n",
      " ('Lady Gaga videography', 0.38808879256248474),\n",
      " ('Selena Gomez', 0.3880593478679657),\n",
      " ('Bad Romance', 0.3870600163936615),\n",
      " ('The Fame', 0.38667652010917664),\n",
      " ('A Very Gaga Thanksgiving', 0.378110408782959),\n",
      " ('Cynthia Germanotta', 0.37442323565483093)]\n"
     ]
    }
   ],
   "source": [
    "#for model in [model_dbow, model_dm]:\n",
    "for model in [model_dm]:\n",
    "    print(model)\n",
    "    vec = [model.dv[\"Lady Gaga\"] + model.wv[\"american\"] - model.wv[\"japanese\"]]\n",
    "    # I switched the search math here...+ american and - japanese\n",
    "    pprint([m for m in model.dv.most_similar(vec, topn=11) if m[0] != \"Lady Gaga\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results demonstrate that both training modes employed in the original paper are outstanding for calculating similarity between document vectors, word vectors, or a combination of both. The DM mode has the added advantage of being 4x faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue your doc2vec explorations, refer to the official API documentation in Gensim: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
