{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Neural/AI Search applied to Proxy Logs</u>\n",
    "\n",
    "The intention with this Jupyter notebook is to examine how we might apply AI search to security logs.  We'll be using fabricated proxy logs that (in theory) show benign behavior, malicious behavior, and a mix of those two. I want to demonstrate the following three examples:\n",
    "- Compare incoming logs to a vector DB of known malicious logs, flagging matches for alert\n",
    "- Compare incoming logs to a vector DB of known benign logs, flagging possible anomalies for alert\n",
    "- Show clustering done with full text via their vector embeddings\n",
    "\n",
    "We'll be using a simple development-grade vector datastore called ChromaDB, and a locally-running HuggingFace LLM (large language model) that is a common one of calculating generic vector embeddings of text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Preparation steps:\n",
    "python -v venv venv\n",
    "venv\\Scripts\\Activate.ps1\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install chromadb\n",
    "pip install sentence-transformers\n",
    "Details from https://github.com/chroma-core/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd \n",
    "import chromadb\n",
    "#from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Flag incoming logs that match known malicious logs</u>\n",
    "## Create vector DB of malicious logs\n",
    "We can use this vectorstore of known malicious logs as reference data to compare incoming logs against to determine if any of those events are similar enough to raise an alert.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CSV file contains fabricated proxy logs that are examples of malicious activity attempts\n",
    "df = pd.read_csv('proxy_logs_malicious.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\n",
    "client = chromadb.Client()\n",
    "#client = chromadb.PersistentClient(path='./chromadb_proxy_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection. get_collection, get_or_create_collection, delete_collection also available\n",
    "# ChromaDB uses L2 (Euclidean distance) by default...we want Cosine metric.\n",
    "# Cosine similarity -> higher = better\n",
    "collection = client.get_or_create_collection(name='malicious_proxy_logs', metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of the necessary data from the dataframe\n",
    "ID_list = df['ID'].astype(str).tolist()  # ID list, converted to string\n",
    "LogEntry_list = df['Log Entry'].tolist()   # List of documents (log content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add docs to the collection. Can also update and delete.\n",
    "# We are letting ChromaDB automatically calculate the vector embedding, instead of explicitly handling it\n",
    "# By default, ChromaDB uses all-MiniLM-L6-v2 sentence transformer model to calculate vector embeddings\n",
    "# This all-MiniLM-L6-v2 model provides a 384 dimension vector that can be used for embedding and clustering\n",
    "collection.add(\n",
    "    documents=LogEntry_list, # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
    "    #metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # metadata filters\n",
    "    ids=ID_list, # unique ID for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "First use of vector embedding triggers download of model:\n",
    "C:\\Users\\david\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:03<00:00, 27.0MiB/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a record by ID...tell it to show the vector embedding so we can see what it looks like\n",
    "# The vector embedding is the 384 dimension numeric representation of what that text \"means\"...\n",
    "collection.get('1', include=['embeddings', 'documents', 'metadatas'])\n",
    "#collection.get(['1','2'], include=['embeddings', 'documents', 'metadatas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine simple calcs around vector distance to foster understanding\n",
    "import numpy\n",
    "import math\n",
    "from scipy import spatial\n",
    "\n",
    "# Grab data from two of the records we've put into the vector DB to examine\n",
    "vec1 = collection.get('1', include=['embeddings', 'documents'])['embeddings'][0]\n",
    "vec2 = collection.get('2', include=['embeddings', 'documents'])['embeddings'][0]\n",
    "\n",
    "# Calc the euclidean (or L2) distance between those two vectors\n",
    "# This is the same thing you'd do with geographic distance between 2 cities\n",
    "# Euclidean provides magnitude but not direction...a lower distance is better match\n",
    "euclidean_dist = spatial.distance.euclidean(vec1, vec2)\n",
    "print('euclidean_dist: ', euclidean_dist)\n",
    "\n",
    "# Cosine similarity is the difference between angles...it provides direction but not magnitude\n",
    "# Cosine is often a good metric for text comparison...not that a higher value is better match\n",
    "cosine_dist = spatial.distance.cosine(vec1, vec2)\n",
    "print('cosine dist: ', cosine_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an ANN Query/search for K most similar results.\n",
    "results = collection.query(\n",
    "    query_texts=[\"http://www.example.com/../../etc/passwd\"], # This gets vectorized and used for vector query\n",
    "    n_results=3,\n",
    "    # where_document={\"$contains\":\"Macintosh\"}  # optional keyword filter\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional metadata filter\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the same question, but this time in plain English!\n",
    "results = collection.query(\n",
    "    query_texts=[\"Can you show me possible attempts to change a password?\"],  # Vectorize and search\n",
    "    n_results=3,\n",
    "    # where_document={\"$contains\":\"script\"}  # optional keyword filter\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional metadata filter\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic matching of malicious attempts\n",
    "We have a vector DB of malicious logs to execute neural searches against.  Now, let's feed it some fresh logs to see what matches we get.  We want to use cosine similarity for this.  We'll have to experiment a bit to set a reasonable threshold for when to return an alert vs not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some fabricated proxy logs that contain 950 benign log entries and 50 malicious log entries\n",
    "df = pd.read_csv('proxy_logs_mixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_proxy_logs = df['Log Entry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_proxy_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incoming log entries that match known malicious activity at a pre-determined threshold \n",
    "for log_entry in incoming_proxy_logs:\n",
    "    results = collection.query(\n",
    "    query_texts=log_entry,\n",
    "    n_results=1,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    "    )\n",
    "    if results['distances'][0][0] >= 0.485:  # Threshold for a match\n",
    "       print(f'Possible alert on: {log_entry} \\n  -- Match within threshold on {results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Anomaly Detection by AI search on Benign Logs</u>\n",
    "## Build vector DB of Benign Proxy log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To conserve memory, re-use the previous collection for new data\n",
    "collection = client.get_or_create_collection(name='benign_proxy_logs', metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CSV file contains fabricated proxy logs that are examples of benign activity\n",
    "df = pd.read_csv('proxy_logs_good.csv')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of the necessary data from the dataframe\n",
    "ID_list = df['ID'].astype(str).tolist()  # ID list, converted to string\n",
    "LogEntry_list = df['Log Entry'].tolist()   # List of documents (log content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log records to vector store, allowing ChromaDB to calculate vector embeddings\n",
    "collection.add(\n",
    "    documents=LogEntry_list, # we handle tokenization, embedding, and indexing automatically. \n",
    "    #metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # metadata filters\n",
    "    ids=ID_list, # unique ID for each doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few records...\n",
    "collection.get(['1','2','3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector search for Anomaly Detection\n",
    "With a vector DB of known benign/good proxy log entries, we can do semantic comparison to flag incoming log entries that are too different from what we know to be benign logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use fabricated proxy logs that contain 950 benign log entries and 50 malicious log entries again\n",
    "df = pd.read_csv('proxy_logs_mixed.csv')\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab just the log entry iself\n",
    "incoming_proxy_logs = df['Log Entry']\n",
    "incoming_proxy_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's identify anomalies in the incoming logs by using vector search against DB of known benign logs\n",
    "for log_entry in incoming_proxy_logs:\n",
    "    results = collection.query(\n",
    "    query_texts=log_entry,\n",
    "    n_results=1,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
    "    )\n",
    "    if results['distances'][0][0] <= 0.0001:  # Threshold for a non-match\n",
    "       print(f'Possible anomaly on: {log_entry} \\n  -- Different within threshold on {results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Full-text Clustering with Vector Embeddings</u>\n",
    "We will use the mixed proxy log data which 950 benign logs + 50 malicious logs to see if we can segregate the two using clustering. This is a notable exercise...only recently has this technology advanced to the point that we can actually do Clustering with full text such as security logs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the mixed proxy logs to see if we can get them clustered into 950 benign + 50 malicious\n",
    "df = pd.read_csv('proxy_logs_mixed.csv')\n",
    "df = df.drop(['IP Address', 'Timestamp'], axis=1)  # We don't need IP nor timestamp for this task\n",
    "df.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's easier to just calc the vector embeddings explicitly, vs adding inserting to ChromaDB to get embeddings\n",
    "# We will use the same embedding model that ChromaDB is using\n",
    "\n",
    "# Load the embedding model so we can use it to easily populate the dataframe\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Use like this: embeddings = model.encode(whatever_text)\n",
    "\n",
    "# Use a lambda function to encode the text in each row and apply it to a new column\n",
    "df['embedding'] = df['Log Entry'].apply(lambda text:model.encode(text))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming you have a DataFrame `df` with columns: 'ID', 'text', and 'embedding'\n",
    "# Here 'embedding' contains the 384-dimensional vectors\n",
    "\n",
    "# Extract the embeddings from the DataFrame\n",
    "embeddings = df['embedding'].tolist()\n",
    "\n",
    "# Convert the list of embeddings into a numpy array\n",
    "# import numpy as np\n",
    "X = np.array(embeddings)\n",
    "\n",
    "# Perform K-means clustering with K=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Now `df` will have a new column 'cluster' indicating the cluster each entry belongs to\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should find 950 in one cluster group and 50 in the other cluster group\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Do we want to try Agglomerative or a different means of Clustering?  Researching this...\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "clustering = AgglomerativeClustering(n_clusters=2,metric=\"cosine\",linkage=\"average\").fit(X)\n",
    "clustering\n",
    "clustering.labels_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
