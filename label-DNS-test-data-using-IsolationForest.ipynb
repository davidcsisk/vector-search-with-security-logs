{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Labeling of Test Data\n",
    "This is an exercise in labeling DNS log test data without using vector embeddings.  I've used one-hot encoding to get a matrix of 0/1's, then PCA dimensionality reduction on that matrix, then trained an isolation forest model with the data, and finally used that model to then label each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 140\n",
      "Sample anomalies:\n",
      "    protocol resolved_address    class query_type response rejected  \\\n",
      "588      udp     version.bind  C_CHAOS        TXT   NOTIMP        T   \n",
      "592      udp     version.bind  C_CHAOS        TXT        -        F   \n",
      "602      udp     version.bind  C_CHAOS        TXT   NOTIMP        T   \n",
      "616      udp     version.bind  C_CHAOS        TXT  NOERROR        F   \n",
      "618      udp     version.bind  C_CHAOS        TXT  NOERROR        F   \n",
      "\n",
      "     raw_score  anomaly_score  \n",
      "588  -0.088306             -1  \n",
      "592  -0.054408             -1  \n",
      "602  -0.088306             -1  \n",
      "616  -0.063306             -1  \n",
      "618  -0.063306             -1  \n"
     ]
    }
   ],
   "source": [
    "# Done with PCA dimensionality reduction for the one-hot encoding\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load your dataset\n",
    "file_path = \"dns-log_test-data.csv\"  # Replace with your file path\n",
    "dns_log_data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: One-Hot Encode Categorical Features\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use sparse=False for dense output\n",
    "encoded_features = encoder.fit_transform(dns_log_data)\n",
    "\n",
    "# Step 2: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=50, random_state=42)  # Reduce dimensions to 50 components\n",
    "reduced_features = pca.fit_transform(encoded_features)\n",
    "\n",
    "# Step 3: Apply Isolation Forest for Anomaly Detection\n",
    "iforest = IsolationForest(n_estimators=100, max_samples=1000, contamination='auto', random_state=42)\n",
    "iforest.fit(reduced_features)\n",
    "\n",
    "# Step 4: Add Anomaly Scores and Flags to Original Dataset\n",
    "dns_log_data['raw_score'] = iforest.decision_function(reduced_features)  # Anomaly score\n",
    "dns_log_data['anomaly_score'] = iforest.predict(reduced_features)       # -1 for anomaly, 1 for normal\n",
    "\n",
    "# Step 5: Separate Anomalies\n",
    "anomalies = dns_log_data[dns_log_data['anomaly_score'] == -1]  # Filter flagged anomalies\n",
    "\n",
    "# Optional: Save results to a new file\n",
    "anomalies.to_csv(\"dns-log_test-data_labeled-anomalies-only.csv\", index=False)\n",
    "dns_log_data.to_csv(\"dns-log_test-data_labeled.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Number of anomalies detected: {anomalies.shape[0]}\")\n",
    "print(\"Sample anomalies:\")\n",
    "print(anomalies.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
